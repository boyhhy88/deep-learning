{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t2yoAReiLVfy"
   },
   "source": [
    "# Neural Machine Translation with Attention Model\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We will build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). We will do this using an attention model, one of the most sophisticated sequence to sequence models.\n",
    "\n",
    "The model we will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give us a place to experiment with these models even without using massive datasets, we will instead use a simpler \"date translation\" task. \n",
    "\n",
    "The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) and translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD.\n",
    "\n",
    "**LSTM**: Long Short-Term Memory unit. The output of an LSTM cell is called the hidden state. Each LSTM cell retains an internal state that is not output, called the cell state, or c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "nPPsGkWyLVf0",
    "outputId": "80756386-1333-4ef5-dd94-7cee487990b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/ed/94a23058daff92545869848ccbcaeb826bc79c6ba4459c7df31ebe1f196d/Faker-2.0.1-py2.py3-none-any.whl (878kB)\n",
      "\u001b[K     |████████████████████████████████| 880kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from faker) (1.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from faker) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.5.3)\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install faker # install package on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K_R6gYxLLVf3",
    "outputId": "d75fa4ff-1189-4e22-8b2a-179330b75384"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from faker import Faker # Faker is a Python package that generates fake data\n",
    "from babel.dates import format_date # for working with date and time information\n",
    "from pprint import pprint\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import RepeatVector, Concatenate, Dense, Activation, Dot, Input \n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYGK3pvhLVf7"
   },
   "outputs": [],
   "source": [
    "Tx = 30 # maximum length of the human readable date\n",
    "Ty = 10 # maximum length of machine readable date (YYYY-MM-DD)\n",
    "m = 40000 # number of training examples to be generated\n",
    "\n",
    "n_a = 32 # dimension of the hidden state vector for the Bi-LSTM\n",
    "n_s = 64 # dimension of the hidden state vector for the post-attention LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdHqvHdxLVf9"
   },
   "source": [
    "## 2. Dataset Generation and Pre-Processing\n",
    "\n",
    "We will train the model on a dataset of m human readable dates and output their equivalent, standardized, machine readable dates.\n",
    "\n",
    "We will use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyILg8IkLVf-"
   },
   "source": [
    "### 2.1 Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6mN0nAVLVf_"
   },
   "source": [
    "Let's take a look at what different formats we will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "EEIoA3FpLVgA",
    "outputId": "e439d503-4d8d-4b7f-bced-90d8becb28e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: 8/8/18\n",
      "EEE: Wed\n",
      "EEEE: Wednesday\n",
      "d: 8\n",
      "dd: 08\n",
      "MM: 08\n",
      "MMM: Aug\n",
      "MMMM: August\n",
      "YY: 18\n",
      "YYYY: 2018\n"
     ]
    }
   ],
   "source": [
    "# show the meaning of different format names\n",
    "\n",
    "formats = ['short', 'EEE', 'EEEE', 'd', 'dd', 'MM', 'MMM', 'MMMM', 'YY', 'YYYY']\n",
    "dt = date(2018, 8, 8)\n",
    "for f in formats:\n",
    "    print(f + ': ', end = '')\n",
    "    human_readable = format_date(dt, format = f, locale = 'en_US')\n",
    "    print(human_readable, end = '')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jr1tPoUdLVgD"
   },
   "source": [
    "Let's create our datesets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yivBv4NZLVgE",
    "outputId": "61ee271a-e270-485f-889a-b25a67e4c028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different human-readable formats we will create: 51\n"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "# Calling the same methods with the same version of faker and seed produces the same results.\n",
    "fake.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "FORMATS_DM = [' d MMM ', ' d MMMM ', ' dd MMM ', ' dd MMMM ', \n",
    "              ' MMM d ', ' MMMM d ', ' MMM dd ', ' MMMM dd ',]\n",
    "FORMATS_DMY = [i + 'YYYY' for i in FORMATS_DM] + [i + 'YY' for i in FORMATS_DM]\n",
    "FORMATS_EDMY = ['EEE' + i for i in FORMATS_DMY] + ['EEEE' + i for i in FORMATS_DMY]\n",
    "FORMATS = ['short'] + FORMATS_DMY + FORMATS_EDMY + ['MM.dd.YY', 'MM.dd.YYYY']\n",
    "print('The number of different human-readable formats we will create: ' + str(len(FORMATS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIrvMoc5LVgH"
   },
   "outputs": [],
   "source": [
    "# function to generate a random date \n",
    "\n",
    "def generate_date():\n",
    "    \"\"\"\n",
    "    Generate a fake date\n",
    "    \n",
    "    Returns: a tuple including human readable string, machine readable string ('YYYY-MM-DD'), \n",
    "    and the corresponding python datetime.date object\n",
    "    \n",
    "    Note that the human readable string is converted to lower case with ',' removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get a fake date object\n",
    "    dt = fake.date_object() # returns a python datetime.date object\n",
    "    \n",
    "    try:\n",
    "        human_readable = format_date(dt, format = random.choice(FORMATS), locale = 'en_US')\n",
    "        human_readable = human_readable.lower()\n",
    "        human_readable = human_readable.replace(',', '')\n",
    "        # Return a string representing the date in ISO 8601 format, ‘YYYY-MM-DD’. \n",
    "        machine_readable = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "    \n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKXwbnLXLVgJ"
   },
   "outputs": [],
   "source": [
    "# function to generate datasets\n",
    "\n",
    "def generate_dataset(m):\n",
    "    \"\"\"\n",
    "    Generate a dataset of m examples and volcabularies\n",
    "    \n",
    "    Arguments:\n",
    "    m: number of examples\n",
    "    \n",
    "    Returns:\n",
    "    dataset: a list of m tuples of (human readable date, machine readable date).\n",
    "    human_vocab: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "    machine_vocab: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "        These indices are not necessarily consistent with human_vocab.\n",
    "    inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters.\n",
    "    \"\"\"\n",
    "        \n",
    "    human_set = set() # store all characters used in the human readable dates\n",
    "    machine_set = set() # store all characters used in the machine readable dates\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for i in tqdm(range(m)):\n",
    "        human_readable, machine_readable, _ = generate_date()\n",
    "        if human_readable is not None:\n",
    "            dataset.append((human_readable, machine_readable))\n",
    "            human_set.update(tuple(human_readable))\n",
    "            machine_set.update(tuple(machine_readable))\n",
    "            \n",
    "    # add '<unk>' and '<pad>' for unknown and padded characters\n",
    "    human_vocab = dict(zip(sorted(human_set) + ['<unk>', '<pad>'], list(range(len(human_set) + 2)))) \n",
    "    machine_vocab = dict(zip(sorted(machine_set), list(range(len(machine_set))))) \n",
    "    inv_machine_vocab = {v:k for k,v in machine_vocab.items()}\n",
    "    \n",
    "    return dataset, human_vocab, machine_vocab, inv_machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5910lQilLVgM",
    "outputId": "24d20280-ca49-4a87-fd61-e17ebd7bb893"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:01<00:00, 27818.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate m datasets\n",
    "\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = generate_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "3pflLKe5LVgP",
    "outputId": "3dc741ec-623d-42f4-c16c-e73cd72eadff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set examples:\n",
      "[('sat 9 may 98', '1998-05-09'),\n",
      " ('thursday september 10 70', '1970-09-10'),\n",
      " ('4/28/90', '1990-04-28'),\n",
      " ('thu 26 jan 1995', '1995-01-26'),\n",
      " ('mon mar 07 1983', '1983-03-07'),\n",
      " (' 22 may 88', '1988-05-22'),\n",
      " ('tue 8 jul 2008', '2008-07-08'),\n",
      " ('wednesday 08 september 1999', '1999-09-08'),\n",
      " ('thu 01 jan 81', '1981-01-01'),\n",
      " (' 22 may 95', '1995-05-22'),\n",
      " ('fri jun 16 1978', '1978-06-16'),\n",
      " (' jun 18 1999', '1999-06-18'),\n",
      " ('thu 08 oct 87', '1987-10-08'),\n",
      " (' april 01 08', '2008-04-01'),\n",
      " ('thursday 06 sep 2012', '2012-09-06'),\n",
      " ('thursday november 12 1981', '1981-11-12'),\n",
      " (' 19 aug 11', '2011-08-19'),\n",
      " ('saturday aug 25 2007', '2007-08-25'),\n",
      " ('saturday 18 sep 1982', '1982-09-18'),\n",
      " (' 25 feb 94', '1994-02-25')]\n"
     ]
    }
   ],
   "source": [
    "# The pprint module provides a capability to “pretty-print” arbitrary Python data structures.\n",
    "print('Data set examples:')\n",
    "pprint(dataset[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "70SshkM7LVgT",
    "outputId": "d41a2f6f-8cc4-4f8b-8b72-f9c3a0ed79f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "human_vocab:\n",
      "{' ': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'y': 34, '<unk>': 35, '<pad>': 36}\n",
      "\n",
      "machine_vocab:\n",
      "{'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10}\n",
      "\n",
      "inv_machine_vocab:\n",
      "{0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9'}\n"
     ]
    }
   ],
   "source": [
    "print('\\nhuman_vocab:')\n",
    "print(human_vocab)\n",
    "print('\\nmachine_vocab:')\n",
    "print(machine_vocab)\n",
    "print('\\ninv_machine_vocab:')\n",
    "print(inv_machine_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-4I4qaULVgW"
   },
   "source": [
    "### 2.2 Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4W7wndSLVgX"
   },
   "source": [
    "Let's preprocess the data and map the raw text data into the index values.\n",
    "\n",
    "We will have:\n",
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpksJmlGLVgY"
   },
   "outputs": [],
   "source": [
    "def string_to_ints(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Convert the characters in string into a list of integer indices based on the dictionary\n",
    "    \n",
    "    Argument:\n",
    "    string: input string, e.g., 'Mon 3 Sep 2018'\n",
    "    length: the number of time steps, which determines if the input will be padded or cut\n",
    "    vocab: vocabulary, dictionary used to convert character to index\n",
    "    \n",
    "    Returns:\n",
    "    output: list of integers representing the indices of string's characters according to vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "        \n",
    "    output = list(map(lambda x : vocab.get(x, vocab.get('<unk>')), tuple(string)))\n",
    "    \n",
    "    if len(string) < length:\n",
    "        output += [vocab['<pad>']] * (length - len(string))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QoERhPYLVga"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"\n",
    "    Convert the characters in strings into one-hot vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    human_readable, machine_readable = zip(*dataset) \n",
    "    \n",
    "    human_readable_ints = np.array([string_to_ints(string, Tx, human_vocab) for string in human_readable]) #(m, Tx)\n",
    "    machine_readable_ints = np.array([string_to_ints(string, Ty, machine_vocab) for string in machine_readable]) #(m, Ty)\n",
    "    \n",
    "    # map() applies the to_categorical() function to each of the m items.\n",
    "    human_readable_oh = np.array(list(map(lambda x : to_categorical(x, num_classes = len(human_vocab)), \n",
    "                                          human_readable_ints))) \n",
    "    machine_readable_oh = np.array(list(map(lambda x : to_categorical(x, num_classes = len(machine_vocab)), \n",
    "                                            machine_readable_ints)))\n",
    "    \n",
    "    return human_readable_ints, machine_readable_ints, human_readable_oh, machine_readable_oh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STpywxCELVgd"
   },
   "outputs": [],
   "source": [
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "WS7DgtLRLVgg",
    "outputId": "6f23ce54-b80e-4403-df4f-fc85b248ff92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (human readable): (40000, 30)\n",
      "Y.shape (machine readable): (40000, 10)\n",
      "Xoh.shape (one-hot): (40000, 30, 37)\n",
      "Yoh.shape (one-hot): (40000, 10, 11)\n",
      "\n",
      "Example 10: \n",
      "human readable date: fri jun 16 1978\n",
      "machine readable date: 1978-06-16\n",
      "human readable date after processing (indices):\n",
      "[18 28 21  0 22 31 25  0  4  9  0  4 12 10 11 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "machine readable date after processing (indices):\n",
      "[ 2 10  8  9  0  1  7  0  2  7]\n",
      "human readable date after processing (one hot): \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "machine readable date after processing (one hot):\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('X.shape (human readable): ' + str(X.shape))\n",
    "print('Y.shape (machine readable): ' + str(Y.shape))\n",
    "print('Xoh.shape (one-hot): ' + str(Xoh.shape))\n",
    "print('Yoh.shape (one-hot): ' + str(Yoh.shape))\n",
    "\n",
    "index = 10\n",
    "print('\\nExample ' + str(index) + ': ')\n",
    "print('human readable date: ' + str(dataset[index][0]))\n",
    "print('machine readable date: ' + str(dataset[index][1]))\n",
    "print('human readable date after processing (indices):')\n",
    "print(X[index])\n",
    "print('machine readable date after processing (indices):')\n",
    "print(Y[index])\n",
    "print('human readable date after processing (one hot): ')\n",
    "print(Xoh[index])\n",
    "print('machine readable date after processing (one hot):')\n",
    "print(Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyI_PMZyLVgj"
   },
   "source": [
    "## 3. Attention Model\n",
    "\n",
    "If we had to translate a book's paragraph from French to English, we would not read the whole paragraph, then close the book and translate. Even during the translation process, we would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English we are writing down. \n",
    "\n",
    "The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "Here is a figure about how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsqTnN3ALVgk"
   },
   "source": [
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIebgQeTLVgl"
   },
   "source": [
    "Here are some properties of the model: \n",
    "\n",
    "- There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. Since we are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. However, unlike text generation examples, in this model the post-activation LSTM at time $t$ will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. \n",
    "\n",
    "- We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. \n",
    "\n",
    "- The diagram on the right uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t' \\rangle}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIKU7WdzLVgm"
   },
   "source": [
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector:\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "We will call the layers in `one_step_attention()` $T_y$ times using a for-loop, and it is important that all dense layers (to calculate attention weights) have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here's how you can implement layers with shareable weights in Keras:\n",
    "1. Define the layer objects (as global variables).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "See one_step_attention() below to understand the dimensions and axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67fcEsVSLVgn"
   },
   "source": [
    "Note: https://jamesmccaffrey.wordpress.com/2016/03/04/the-max-trick-when-computing-softmax/ <br>\n",
    "In practice, calculating softmax values can go wrong if any x value is very large — the exp() of even a moderate-magnitude positive number can be astronomically huge. A trick to avoid this computation problem is subtract the largest x value from each x value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uuGFx0fBLVgo"
   },
   "outputs": [],
   "source": [
    "# our customized softmax function\n",
    "# The dimension of input x will be (m, Tx, 1), and softmax will be applied to axis = 1\n",
    "\n",
    "def softmax(x, axis = 1):\n",
    "    \"\"\"\n",
    "    compute softmax activation\n",
    "    \n",
    "    Arguments:\n",
    "    x: input tensor\n",
    "    axis: integer, axis along which the softmax normalization is applied\n",
    "    \n",
    "    Return: tensor, output of softmax activation, with the same dimension as x\n",
    "    \"\"\"\n",
    "    \n",
    "    max_val = K.max(x, axis = axis, keepdims = True)\n",
    "    e = K.exp(x - max_val)\n",
    "    s = K.sum(e, axis = axis, keepdims = True)\n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMTEbFFiLVgq"
   },
   "outputs": [],
   "source": [
    "# define shared layers as global variables\n",
    "\n",
    "# Dense(): input shape: (batch_size, ..., input_dim), output shape: (batch_size, ..., units)\n",
    "densor1 = Dense(units = 10, activation = 'tanh', name = 'context_dense1')\n",
    "densor2 = Dense(units = 1, name = 'context_dense2')\n",
    "repeator = RepeatVector(Tx, name = 'repeat_s')\n",
    "concatenator = Concatenate(axis = -1, name = 'concatenate_s_a')\n",
    "activator = Activation(softmax, name = 'attention_weights') # use our customized softmax function above.\n",
    "dotor = Dot(axes = 1, name = 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHGFeuegLVgu"
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Compute the context vector, using dot product of alpha<t, t'> and a<t'> \n",
    "    \n",
    "    Argument:\n",
    "    a: all of the hidden state output a<t'> of the Bi-LSTM, numpy array with dimension of (m, Tx, 2 * n_a)\n",
    "    s_prev: previous hiddent state s<t-1> of the post-attention LSTM, numpy array with dimension of (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context: context vector, weighted sum of a<t'>, input of post-attention LSTM cell, with dimension of (m, 1, 2 * n_a)\n",
    "    \"\"\"\n",
    "    \n",
    "    # repeat s_prev so the shape is (m, Tx, n_s), so that we can concatenate it with \"a\"\n",
    "    s_prev = repeator(s_prev)\n",
    "    # concatenate a and s_prev on the last axis, so the shape is (m, Tx, 2 * n_a + n_s)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e, \n",
    "    # with dimension (m, Tx, 10)\n",
    "    e = densor1(concat)\n",
    "    # propagate \"e\" through a small fully-connected neural network to compute the \"energies\" variable energies, \n",
    "    # with dimension (m, Tx, 1)\n",
    "    energies = densor2(e)\n",
    "    # use softmax activation on \"energies\" to compute the attention weights \"alphas\", with dimension of (m, Tx, 1)\n",
    "    alphas = activator(energies)\n",
    "    # compute the context vector, with the dimension of (m, 1, 2 * n_a)\n",
    "    # dot product of (m, Tx, 1) and (m, Tx, 2 * n_a), along axis = 1, is (m, 1, 2 * n_a). \n",
    "    # Remember that dot product of (batch_size, n) and (batch_size, n) is (batch_size, 1).\n",
    "    # each row t' of alphas(m, Tx, 1) is alpha<t, t'> (1, 1), and each row of a(m, Tx, 2*n_a) is a<t'> (1, 2*n_a), \n",
    "    # apply dot product will multiply each of the 2*n_a items in a<t'> by alpha<t, t'>, and then sum over Tx \n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmc5UoArLVgz"
   },
   "source": [
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. \n",
    "\n",
    "Again, we have defined global layers that will share weights to be used in `model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzE9wPs-LVg1"
   },
   "source": [
    "**return_state and return_sequences in LSTM():**<br>\n",
    "https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/<br>\n",
    "return_state argument in the LSTM layer will provide access to not only the hidden state output but also the cell state.\n",
    "\n",
    "If we set return_sequences = True, return_state = False, it will return:\n",
    "- a sequence of timesteps values, one hidden state output for each input time step for the single LSTM cell in the layer.\n",
    "\n",
    "If we set return_sequences = False, return_state = True, it will return 3 arrays:\n",
    "- The LSTM hidden state output for the last input time step.\n",
    "- The LSTM hidden state output for the last input time step (again).\n",
    "- The LSTM cell state for the last input time step.\n",
    "\n",
    "If we set return_sequences = True, return_state = True, it will return 3 arrays:\n",
    "- The LSTM hidden state output for each input time step.\n",
    "- The LSTM hidden state output for the last input time step.\n",
    "- The LSTM cell state for the last input time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhMQYC5rLVg3"
   },
   "outputs": [],
   "source": [
    "post_attention_LSTM = LSTM(units = n_s, return_state = True, name = 'post_attention_LSTM_cell')\n",
    "output_layer = Dense(len(machine_vocab), activation = softmax, name = 'post_attention_LSTM_dense_softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAyzZ4pyLVg6"
   },
   "source": [
    "We will carry out the following steps: \n",
    "\n",
    "1. Propagate the input into a [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "2. Iterate for $t = 0, \\dots, T_y-1$: \n",
    "    1. Call `one_step_attention()` on $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ and $s^{<t-1>}$ to get the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM using `initial_state= [previous hidden state, previous cell state]`. Get back the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.\n",
    "    3. Apply a softmax layer to $s^{<t>}$, get the output. \n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create our Keras model instance, it should have three inputs (\"inputs\", $s^{<0>}$ and $c^{<0>}$) and output the list of \"outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAmMYT_ZLVg7"
   },
   "outputs": [],
   "source": [
    "def nmt_model(Tx, Ty, n_a, n_s, input_one_hot_size):\n",
    "    \"\"\"\n",
    "    complete neural machine translation model\n",
    "    \n",
    "    Arguments:\n",
    "    Tx: length of the input sequence\n",
    "    Ty: length of the output sequence\n",
    "    n_a: dimension of the hidden state vector for the Bi-LSTM\n",
    "    n_s: dimension of the hidden state vector and cell state vector for the post-attention LSTM\n",
    "    input_one_hot_size: length of the one-hot vector of the input\n",
    "    \n",
    "    Returns:\n",
    "    model: Keras model instance\n",
    "    \"\"\"\n",
    "    # input layer, the batch dimension is not included in the shape.\n",
    "    X = Input(shape = (Tx, input_one_hot_size), name = 'input') # shape: (m, Tx, len(human_vocab))\n",
    "    # s0 and c0 are initial hidden and cell states for the post-attention LSTM (decoder), \n",
    "    # which are vectors with dimension of (n_s, )\n",
    "    s0 = Input(shape = (n_s, ), name = 's0') # shape: (m, n_s)\n",
    "    c0 = Input(shape = (n_s, ), name = 'c0') # shape: (m, n_s)\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # store the outputs (y hat)\n",
    "    outputs = []\n",
    "    \n",
    "    # pre-attention Bi-LSTM\n",
    "    a = Bidirectional(LSTM(units = n_a, return_sequences = True), name = 'Bi-LSTM')(X) # output shape: (m, Tx, 2 * n_a)\n",
    "    \n",
    "    # iterate for Ty step\n",
    "    for t in range(Ty):\n",
    "        # Call one_step_attention() to get the context vector.\n",
    "        context = one_step_attention(a, s) # output shape: (m, 1, 2 * n_a)\n",
    "        # Give context vector to the post-attention LSTM cell. Get back the new hidden state s<t>  \n",
    "        # and the new cell state c<t>.\n",
    "        s, _, c = post_attention_LSTM(context, initial_state = [s, c]) # both ouput shapes: (m, n_s)\n",
    "        # Apply a dense and softmax layer to s<t>, get the output.\n",
    "        out = output_layer(s) # output shape: (m, len(machine_vocab))\n",
    "        outputs.append(out) \n",
    "    \n",
    "    # after Ty iterations, outputs is a list of Ty Numpy arrays, each with shape (m, len(machine_vocab))\n",
    "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yyE3_5ULVhA"
   },
   "outputs": [],
   "source": [
    "model = nmt_model(Tx, Ty, n_a, n_s, len(human_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ki-NzwGBLVhD",
    "outputId": "461d17d5-e9a7-4e8d-df29-46b42aa71818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Bi-LSTM (Bidirectional)         (None, 30, 64)       17920       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "repeat_s (RepeatVector)         (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 post_attention_LSTM_cell[0][0]   \n",
      "                                                                 post_attention_LSTM_cell[1][0]   \n",
      "                                                                 post_attention_LSTM_cell[2][0]   \n",
      "                                                                 post_attention_LSTM_cell[3][0]   \n",
      "                                                                 post_attention_LSTM_cell[4][0]   \n",
      "                                                                 post_attention_LSTM_cell[5][0]   \n",
      "                                                                 post_attention_LSTM_cell[6][0]   \n",
      "                                                                 post_attention_LSTM_cell[7][0]   \n",
      "                                                                 post_attention_LSTM_cell[8][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_s_a (Concatenate)   (None, 30, 128)      0           Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[0][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[1][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[2][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[3][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[4][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[5][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[6][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[7][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[8][0]                   \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 repeat_s[9][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "context_dense1 (Dense)          (None, 30, 10)       1290        concatenate_s_a[0][0]            \n",
      "                                                                 concatenate_s_a[1][0]            \n",
      "                                                                 concatenate_s_a[2][0]            \n",
      "                                                                 concatenate_s_a[3][0]            \n",
      "                                                                 concatenate_s_a[4][0]            \n",
      "                                                                 concatenate_s_a[5][0]            \n",
      "                                                                 concatenate_s_a[6][0]            \n",
      "                                                                 concatenate_s_a[7][0]            \n",
      "                                                                 concatenate_s_a[8][0]            \n",
      "                                                                 concatenate_s_a[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "context_dense2 (Dense)          (None, 30, 1)        11          context_dense1[0][0]             \n",
      "                                                                 context_dense1[1][0]             \n",
      "                                                                 context_dense1[2][0]             \n",
      "                                                                 context_dense1[3][0]             \n",
      "                                                                 context_dense1[4][0]             \n",
      "                                                                 context_dense1[5][0]             \n",
      "                                                                 context_dense1[6][0]             \n",
      "                                                                 context_dense1[7][0]             \n",
      "                                                                 context_dense1[8][0]             \n",
      "                                                                 context_dense1[9][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           context_dense2[0][0]             \n",
      "                                                                 context_dense2[1][0]             \n",
      "                                                                 context_dense2[2][0]             \n",
      "                                                                 context_dense2[3][0]             \n",
      "                                                                 context_dense2[4][0]             \n",
      "                                                                 context_dense2[5][0]             \n",
      "                                                                 context_dense2[6][0]             \n",
      "                                                                 context_dense2[7][0]             \n",
      "                                                                 context_dense2[8][0]             \n",
      "                                                                 context_dense2[9][0]             \n",
      "__________________________________________________________________________________________________\n",
      "context (Dot)                   (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 Bi-LSTM[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "post_attention_LSTM_cell (LSTM) [(None, 64), (None,  33024       context[0][0]                    \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 context[1][0]                    \n",
      "                                                                 post_attention_LSTM_cell[0][0]   \n",
      "                                                                 post_attention_LSTM_cell[0][2]   \n",
      "                                                                 context[2][0]                    \n",
      "                                                                 post_attention_LSTM_cell[1][0]   \n",
      "                                                                 post_attention_LSTM_cell[1][2]   \n",
      "                                                                 context[3][0]                    \n",
      "                                                                 post_attention_LSTM_cell[2][0]   \n",
      "                                                                 post_attention_LSTM_cell[2][2]   \n",
      "                                                                 context[4][0]                    \n",
      "                                                                 post_attention_LSTM_cell[3][0]   \n",
      "                                                                 post_attention_LSTM_cell[3][2]   \n",
      "                                                                 context[5][0]                    \n",
      "                                                                 post_attention_LSTM_cell[4][0]   \n",
      "                                                                 post_attention_LSTM_cell[4][2]   \n",
      "                                                                 context[6][0]                    \n",
      "                                                                 post_attention_LSTM_cell[5][0]   \n",
      "                                                                 post_attention_LSTM_cell[5][2]   \n",
      "                                                                 context[7][0]                    \n",
      "                                                                 post_attention_LSTM_cell[6][0]   \n",
      "                                                                 post_attention_LSTM_cell[6][2]   \n",
      "                                                                 context[8][0]                    \n",
      "                                                                 post_attention_LSTM_cell[7][0]   \n",
      "                                                                 post_attention_LSTM_cell[7][2]   \n",
      "                                                                 context[9][0]                    \n",
      "                                                                 post_attention_LSTM_cell[8][0]   \n",
      "                                                                 post_attention_LSTM_cell[8][2]   \n",
      "__________________________________________________________________________________________________\n",
      "post_attention_LSTM_dense_softm (None, 11)           715         post_attention_LSTM_cell[0][0]   \n",
      "                                                                 post_attention_LSTM_cell[1][0]   \n",
      "                                                                 post_attention_LSTM_cell[2][0]   \n",
      "                                                                 post_attention_LSTM_cell[3][0]   \n",
      "                                                                 post_attention_LSTM_cell[4][0]   \n",
      "                                                                 post_attention_LSTM_cell[5][0]   \n",
      "                                                                 post_attention_LSTM_cell[6][0]   \n",
      "                                                                 post_attention_LSTM_cell[7][0]   \n",
      "                                                                 post_attention_LSTM_cell[8][0]   \n",
      "                                                                 post_attention_LSTM_cell[9][0]   \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "colab_type": "code",
    "id": "g6wW6nQZLVhG",
    "outputId": "5b9733f3-2b0b-4eb5-c0e6-6d5a766f936d"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 679.00 629.00\" width=\"679pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 675,-625 675,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140129111855344 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140129111855344</title>\n",
       "<polygon fill=\"none\" points=\"166,-584.5 166,-620.5 284,-620.5 284,-584.5 166,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225\" y=\"-598.8\">input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140129111837608 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140129111837608</title>\n",
       "<polygon fill=\"none\" points=\"99.5,-511.5 99.5,-547.5 350.5,-547.5 350.5,-511.5 99.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225\" y=\"-525.8\">Bi-LSTM(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 140129111855344&#45;&gt;140129111837608 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140129111855344-&gt;140129111837608</title>\n",
       "<path d=\"M225,-584.4551C225,-576.3828 225,-566.6764 225,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"228.5001,-557.5903 225,-547.5904 221.5001,-557.5904 228.5001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129111855512 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140129111855512</title>\n",
       "<polygon fill=\"none\" points=\"74.5,-146.5 74.5,-182.5 175.5,-182.5 175.5,-146.5 74.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125\" y=\"-160.8\">s0: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140129129313952 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140129129313952</title>\n",
       "<polygon fill=\"none\" points=\"0,-.5 0,-36.5 146,-36.5 146,-.5 0,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"73\" y=\"-14.8\">repeat_s: RepeatVector</text>\n",
       "</g>\n",
       "<!-- 140129111855512&#45;&gt;140129129313952 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140129111855512-&gt;140129129313952</title>\n",
       "<path d=\"M118.5193,-146.3042C109.6057,-121.2775 93.479,-75.9988 83.0088,-46.6017\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"86.2147,-45.1711 79.5624,-36.9251 79.6205,-47.5198 86.2147,-45.1711\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140129109941384</title>\n",
       "<polygon fill=\"none\" points=\"129,-73.5 129,-109.5 347,-109.5 347,-73.5 129,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238\" y=\"-87.8\">post_attention_LSTM_cell: LSTM</text>\n",
       "</g>\n",
       "<!-- 140129111855512&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge83\">\n",
       "<title>140129111855512-&gt;140129109941384</title>\n",
       "<path d=\"M152.9326,-146.4551C167.601,-136.979 185.7556,-125.2508 201.5214,-115.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"203.4966,-117.9566 209.9971,-109.5904 199.6982,-112.0769 203.4966,-117.9566\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109939760 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140129109939760</title>\n",
       "<polygon fill=\"none\" points=\"72,-438.5 72,-474.5 258,-474.5 258,-438.5 72,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-452.8\">concatenate_s_a: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140129111837608&#45;&gt;140129109939760 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140129111837608-&gt;140129109939760</title>\n",
       "<path d=\"M210.1685,-511.4551C202.9569,-502.6809 194.1583,-491.9759 186.249,-482.353\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"188.9224,-480.0934 179.8688,-474.5904 183.5146,-484.5382 188.9224,-480.0934\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109939536 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140129109939536</title>\n",
       "<polygon fill=\"none\" points=\"194,-146.5 194,-182.5 282,-182.5 282,-146.5 194,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238\" y=\"-160.8\">context: Dot</text>\n",
       "</g>\n",
       "<!-- 140129111837608&#45;&gt;140129109939536 -->\n",
       "<g class=\"edge\" id=\"edge63\">\n",
       "<title>140129111837608-&gt;140129109939536</title>\n",
       "<path d=\"M242.4747,-511.2204C251.0921,-501.2411 260.916,-488.2428 267,-475 284.3393,-437.2583 286,-425.0342 286,-383.5 286,-383.5 286,-383.5 286,-310.5 286,-268.9658 281.1199,-258.0604 267,-219 263.6285,-209.6732 258.9055,-199.9686 254.2485,-191.3777\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"257.2427,-189.5617 249.2881,-182.5634 251.1424,-192.9948 257.2427,-189.5617\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129129313952&#45;&gt;140129109939760 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140129129313952-&gt;140129109939760</title>\n",
       "<path d=\"M67.4056,-36.7376C59.4971,-64.1779 46,-117.8436 46,-164.5 46,-310.5 46,-310.5 46,-310.5 46,-354.3657 55.2733,-368.0084 83,-402 93.0153,-414.2783 106.5287,-424.8104 119.6939,-433.251\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"117.8988,-436.2558 128.2547,-438.493 121.5542,-430.2861 117.8988,-436.2558\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129111857080 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140129111857080</title>\n",
       "<polygon fill=\"none\" points=\"91.5,-365.5 91.5,-401.5 240.5,-401.5 240.5,-365.5 91.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-379.8\">context_dense1: Dense</text>\n",
       "</g>\n",
       "<!-- 140129109939760&#45;&gt;140129111857080 -->\n",
       "<g class=\"edge\" id=\"edge32\">\n",
       "<title>140129109939760-&gt;140129111857080</title>\n",
       "<path d=\"M165.2472,-438.4551C165.3578,-430.3828 165.4907,-420.6764 165.6139,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"169.1148,-411.6374 165.7522,-401.5904 162.1154,-411.5414 169.1148,-411.6374\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129111857248 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140129111857248</title>\n",
       "<polygon fill=\"none\" points=\"91.5,-292.5 91.5,-328.5 240.5,-328.5 240.5,-292.5 91.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-306.8\">context_dense2: Dense</text>\n",
       "</g>\n",
       "<!-- 140129111857080&#45;&gt;140129111857248 -->\n",
       "<g class=\"edge\" id=\"edge42\">\n",
       "<title>140129111857080-&gt;140129111857248</title>\n",
       "<path d=\"M166,-365.4551C166,-357.3828 166,-347.6764 166,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"169.5001,-338.5903 166,-328.5904 162.5001,-338.5904 169.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109939648 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140129109939648</title>\n",
       "<polygon fill=\"none\" points=\"73.5,-219.5 73.5,-255.5 258.5,-255.5 258.5,-219.5 73.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-233.8\">attention_weights: Activation</text>\n",
       "</g>\n",
       "<!-- 140129111857248&#45;&gt;140129109939648 -->\n",
       "<g class=\"edge\" id=\"edge52\">\n",
       "<title>140129111857248-&gt;140129109939648</title>\n",
       "<path d=\"M166,-292.4551C166,-284.3828 166,-274.6764 166,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"169.5001,-265.5903 166,-255.5904 162.5001,-265.5904 169.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109939648&#45;&gt;140129109939536 -->\n",
       "<g class=\"edge\" id=\"edge62\">\n",
       "<title>140129109939648-&gt;140129109939536</title>\n",
       "<path d=\"M183.7978,-219.4551C192.6248,-210.5054 203.4331,-199.547 213.0693,-189.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"215.6272,-192.1678 220.1575,-182.5904 210.6434,-187.2523 215.6272,-192.1678\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109939536&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge82\">\n",
       "<title>140129109939536-&gt;140129109941384</title>\n",
       "<path d=\"M238,-146.4551C238,-138.3828 238,-128.6764 238,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"241.5001,-119.5903 238,-109.5904 234.5001,-119.5904 241.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129111857696 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140129111857696</title>\n",
       "<polygon fill=\"none\" points=\"300.5,-146.5 300.5,-182.5 401.5,-182.5 401.5,-146.5 300.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351\" y=\"-160.8\">c0: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140129111857696&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge84\">\n",
       "<title>140129111857696-&gt;140129109941384</title>\n",
       "<path d=\"M323.0674,-146.4551C308.399,-136.979 290.2444,-125.2508 274.4786,-115.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"276.3018,-112.0769 266.0029,-109.5904 272.5034,-117.9566 276.3018,-112.0769\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129129313952 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140129109941384-&gt;140129129313952</title>\n",
       "<path d=\"M197.2135,-73.4551C174.8033,-63.5403 146.8206,-51.16 123.0911,-40.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.4502,-37.4356 113.8892,-36.5904 121.618,-43.8371 124.4502,-37.4356\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge86\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.291,-92.7231C358.1154,-92.478 365,-92.0703 365,-91.5 365,-91.1346 362.1746,-90.836 357.2979,-90.6042\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.4001,-87.1057 347.291,-90.2769 357.1712,-94.102 357.4001,-87.1057\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge87\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.0789,-94.3615C368.102,-94.0242 383,-93.0703 383,-91.5 383,-90.1996 372.7831,-89.3219 357.3315,-88.8671\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.1544,-85.3623 347.0789,-88.6385 356.9983,-92.3606 357.1544,-85.3623\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge89\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.3155,-95.9752C377.7162,-95.7691 401,-94.2773 401,-91.5 401,-89.0373 382.6927,-87.5854 357.3495,-87.1443\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.3565,-83.6443 347.3155,-87.0248 357.273,-90.6438 357.3565,-83.6443\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge90\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.121,-97.4926C386.675,-97.6382 419,-95.6406 419,-91.5 419,-87.7152 391.9921,-85.721 357.1424,-85.5174\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.1245,-82.0174 347.121,-85.5074 357.1175,-89.0174 357.1245,-82.0174\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge92\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.2459,-98.8935C395.4596,-99.5602 437,-97.0957 437,-91.5 437,-86.2977 401.0958,-83.8018 357.3048,-84.0123\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.2126,-80.5129 347.2459,-84.1065 357.2782,-87.5126 357.2126,-80.5129\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge93\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.1413,-100.177C403.8314,-101.529 455,-98.6367 455,-91.5 455,-84.7954 409.8401,-81.8368 357.3677,-82.6241\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.0713,-79.1291 347.1413,-82.823 357.2075,-86.1278 357.0713,-79.1291\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge95\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.0841,-101.354C411.9932,-103.521 473,-100.2363 473,-91.5 473,-83.2244 418.2578,-79.8406 357.312,-81.3487\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"356.9781,-77.8568 347.0841,-81.646 357.1816,-84.8539 356.9781,-77.8568\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge96\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.2218,-102.4424C420.0805,-105.5225 491,-101.875 491,-91.5 491,-81.6113 426.5734,-77.8343 357.4477,-80.1688\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.0816,-76.6801 347.2218,-80.5576 357.3476,-83.6751 357.0816,-76.6801\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge98\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.2312,-103.4408C427.9176,-107.5389 509,-103.5586 509,-91.5 509,-79.9595 434.736,-75.8181 357.6279,-79.0756\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.0578,-75.5983 347.2312,-79.5592 357.383,-82.5907 357.0578,-75.5983\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge99\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.3397,-104.3686C435.676,-109.5591 527,-105.2695 527,-91.5 527,-78.2549 442.4999,-73.7814 357.4406,-78.0796\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.1338,-74.591 347.3397,-78.6314 357.5157,-81.5806 357.1338,-74.591\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge101\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.3684,-105.2242C443.253,-111.5865 545,-107.0117 545,-91.5 545,-76.5336 450.2813,-71.7487 357.4983,-77.1453\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.1316,-73.6613 347.3684,-77.7758 357.5665,-80.6477 357.1316,-73.6613\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge102\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.0429,-105.9929C450.4605,-113.6278 563,-108.7969 563,-91.5 563,-74.7605 457.5972,-69.6965 357.0656,-76.3078\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"356.775,-72.8195 347.0429,-77.0071 357.2623,-79.8025 356.775,-72.8195\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge104\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.324,-106.751C458.0063,-115.652 581,-110.5684 581,-91.5 581,-73.0089 465.3405,-67.6686 357.4063,-75.4791\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.0284,-71.9977 347.324,-76.249 357.5615,-78.9773 357.0284,-71.9977\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge105\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.0241,-107.4128C465.0315,-117.6949 599,-112.3906 599,-91.5 599,-71.201 472.5121,-65.6181 357.0851,-74.7511\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"356.6998,-71.271 347.0241,-75.5872 357.2796,-78.247 356.6998,-71.271\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge107\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.1197,-108.0632C472.2724,-119.7261 617,-114.2051 617,-91.5 617,-69.4158 480.079,-63.5886 357.4267,-74.0186\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"356.7697,-70.5632 347.1197,-74.9368 357.3909,-77.5356 356.7697,-70.5632\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge108\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.2381,-108.6767C479.4612,-121.7568 635,-116.0313 635,-91.5 635,-67.5916 487.2594,-61.5461 357.3519,-73.3634\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"356.8626,-69.894 347.2381,-74.3233 357.5241,-76.8627 356.8626,-69.894\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge110\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M347.1839,-109.2369C486.45,-123.7922 653,-117.8799 653,-91.5 653,-65.7642 494.483,-59.5087 357.43,-72.7337\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"356.7839,-69.2809 347.1839,-73.7631 357.4837,-76.2458 356.7839,-69.2809\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941384 -->\n",
       "<g class=\"edge\" id=\"edge111\">\n",
       "<title>140129109941384-&gt;140129109941384</title>\n",
       "<path d=\"M344.8106,-109.5063C491.4928,-125.8908 671,-119.8887 671,-91.5 671,-63.7767 499.8086,-57.4029 355.1755,-72.3786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"354.3788,-68.944 344.8106,-73.4937 355.1276,-75.9038 354.3788,-68.944\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140129109941608 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140129109941608</title>\n",
       "<polygon fill=\"none\" points=\"164,-.5 164,-36.5 444,-36.5 444,-.5 164,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304\" y=\"-14.8\">post_attention_LSTM_dense_softmax: Dense</text>\n",
       "</g>\n",
       "<!-- 140129109941384&#45;&gt;140129109941608 -->\n",
       "<g class=\"edge\" id=\"edge112\">\n",
       "<title>140129109941384-&gt;140129109941608</title>\n",
       "<path d=\"M254.3146,-73.4551C262.3267,-64.5932 272.1196,-53.7616 280.8868,-44.0646\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"283.5341,-46.3554 287.6443,-36.5904 278.3416,-41.6609 283.5341,-46.3554\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the model schematics\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "El6CIaLPLVhJ"
   },
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "The last step is to define all inputs and outputs to fit the model:\n",
    "- We already have X of shape $(m, Tx)$ containing the training examples.\n",
    "- We need to create `s0` and `c0` to initialize our `post_attention_LSTM` with 0s.\n",
    "- Given the `model()`, we need the \"outputs\" to be a list of Ty elements of shape (m, len(machine_vocab))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcJWyZtLLVhK"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "\n",
    "opt = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999, decay = 0.01)\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Kxn5aZhLVhM"
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "\n",
    "train_x = Xoh\n",
    "# Yoh is (m, Ty, len(machine_vocab)), swap axis 0 and 1 so it is (Ty, m, len(machine_vocab))\n",
    "# convert the first axis to list, to match the output format (a list of Ty arrays)\n",
    "train_y = list(Yoh.swapaxes(0, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "f-aUViQuLVhP",
    "outputId": "607168f3-21f2-4ae3-cefd-8110b55ebdf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 38s - loss: 12.1342 - post_attention_LSTM_dense_softmax_loss: 2.0468 - post_attention_LSTM_dense_softmax_acc: 0.6329 - post_attention_LSTM_dense_softmax_acc_1: 0.7600 - post_attention_LSTM_dense_softmax_acc_2: 0.4589 - post_attention_LSTM_dense_softmax_acc_3: 0.1943 - post_attention_LSTM_dense_softmax_acc_4: 0.9833 - post_attention_LSTM_dense_softmax_acc_5: 0.6526 - post_attention_LSTM_dense_softmax_acc_6: 0.2423 - post_attention_LSTM_dense_softmax_acc_7: 0.9884 - post_attention_LSTM_dense_softmax_acc_8: 0.4941 - post_attention_LSTM_dense_softmax_acc_9: 0.2654\n",
      "Epoch 2/200\n",
      " - 28s - loss: 4.8067 - post_attention_LSTM_dense_softmax_loss: 1.0386 - post_attention_LSTM_dense_softmax_acc: 0.9416 - post_attention_LSTM_dense_softmax_acc_1: 0.9476 - post_attention_LSTM_dense_softmax_acc_2: 0.7817 - post_attention_LSTM_dense_softmax_acc_3: 0.6483 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9810 - post_attention_LSTM_dense_softmax_acc_6: 0.7280 - post_attention_LSTM_dense_softmax_acc_7: 0.9998 - post_attention_LSTM_dense_softmax_acc_8: 0.7535 - post_attention_LSTM_dense_softmax_acc_9: 0.6314\n",
      "Epoch 3/200\n",
      " - 28s - loss: 2.1113 - post_attention_LSTM_dense_softmax_loss: 0.4820 - post_attention_LSTM_dense_softmax_acc: 0.9829 - post_attention_LSTM_dense_softmax_acc_1: 0.9928 - post_attention_LSTM_dense_softmax_acc_2: 0.9526 - post_attention_LSTM_dense_softmax_acc_3: 0.9546 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9917 - post_attention_LSTM_dense_softmax_acc_6: 0.9226 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.8307 - post_attention_LSTM_dense_softmax_acc_9: 0.8294\n",
      "Epoch 4/200\n",
      " - 28s - loss: 1.2287 - post_attention_LSTM_dense_softmax_loss: 0.3112 - post_attention_LSTM_dense_softmax_acc: 0.9979 - post_attention_LSTM_dense_softmax_acc_1: 0.9997 - post_attention_LSTM_dense_softmax_acc_2: 0.9969 - post_attention_LSTM_dense_softmax_acc_3: 0.9942 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9938 - post_attention_LSTM_dense_softmax_acc_6: 0.9719 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.8515 - post_attention_LSTM_dense_softmax_acc_9: 0.8777\n",
      "Epoch 5/200\n",
      " - 28s - loss: 0.9025 - post_attention_LSTM_dense_softmax_loss: 0.2341 - post_attention_LSTM_dense_softmax_acc: 0.9996 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9994 - post_attention_LSTM_dense_softmax_acc_3: 0.9954 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9947 - post_attention_LSTM_dense_softmax_acc_6: 0.9851 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.8726 - post_attention_LSTM_dense_softmax_acc_9: 0.9098\n",
      "Epoch 6/200\n",
      " - 28s - loss: 0.7108 - post_attention_LSTM_dense_softmax_loss: 0.1766 - post_attention_LSTM_dense_softmax_acc: 0.9997 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9954 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9955 - post_attention_LSTM_dense_softmax_acc_6: 0.9901 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.8963 - post_attention_LSTM_dense_softmax_acc_9: 0.9412\n",
      "Epoch 7/200\n",
      " - 28s - loss: 0.5648 - post_attention_LSTM_dense_softmax_loss: 0.1275 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9963 - post_attention_LSTM_dense_softmax_acc_6: 0.9927 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9211 - post_attention_LSTM_dense_softmax_acc_9: 0.9702\n",
      "Epoch 8/200\n",
      " - 28s - loss: 0.4469 - post_attention_LSTM_dense_softmax_loss: 0.0884 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9969 - post_attention_LSTM_dense_softmax_acc_6: 0.9948 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9442 - post_attention_LSTM_dense_softmax_acc_9: 0.9872\n",
      "Epoch 9/200\n",
      " - 28s - loss: 0.3574 - post_attention_LSTM_dense_softmax_loss: 0.0620 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9972 - post_attention_LSTM_dense_softmax_acc_6: 0.9956 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9592 - post_attention_LSTM_dense_softmax_acc_9: 0.9948\n",
      "Epoch 10/200\n",
      " - 28s - loss: 0.2911 - post_attention_LSTM_dense_softmax_loss: 0.0463 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9976 - post_attention_LSTM_dense_softmax_acc_6: 0.9965 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9712 - post_attention_LSTM_dense_softmax_acc_9: 0.9975\n",
      "Epoch 11/200\n",
      " - 28s - loss: 0.2414 - post_attention_LSTM_dense_softmax_loss: 0.0351 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9978 - post_attention_LSTM_dense_softmax_acc_6: 0.9971 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9788 - post_attention_LSTM_dense_softmax_acc_9: 0.9988\n",
      "Epoch 12/200\n",
      " - 28s - loss: 0.2040 - post_attention_LSTM_dense_softmax_loss: 0.0277 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9981 - post_attention_LSTM_dense_softmax_acc_6: 0.9974 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9852 - post_attention_LSTM_dense_softmax_acc_9: 0.9994\n",
      "Epoch 13/200\n",
      " - 28s - loss: 0.1754 - post_attention_LSTM_dense_softmax_loss: 0.0225 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9984 - post_attention_LSTM_dense_softmax_acc_6: 0.9978 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9895 - post_attention_LSTM_dense_softmax_acc_9: 0.9995\n",
      "Epoch 14/200\n",
      " - 28s - loss: 0.1537 - post_attention_LSTM_dense_softmax_loss: 0.0187 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9985 - post_attention_LSTM_dense_softmax_acc_6: 0.9982 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9927 - post_attention_LSTM_dense_softmax_acc_9: 0.9997\n",
      "Epoch 15/200\n",
      " - 28s - loss: 0.1364 - post_attention_LSTM_dense_softmax_loss: 0.0157 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9986 - post_attention_LSTM_dense_softmax_acc_6: 0.9984 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9948 - post_attention_LSTM_dense_softmax_acc_9: 0.9998\n",
      "Epoch 16/200\n",
      " - 28s - loss: 0.1224 - post_attention_LSTM_dense_softmax_loss: 0.0134 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9990 - post_attention_LSTM_dense_softmax_acc_6: 0.9987 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9966 - post_attention_LSTM_dense_softmax_acc_9: 0.9999\n",
      "Epoch 17/200\n",
      " - 28s - loss: 0.1115 - post_attention_LSTM_dense_softmax_loss: 0.0116 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9989 - post_attention_LSTM_dense_softmax_acc_6: 0.9990 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9980 - post_attention_LSTM_dense_softmax_acc_9: 0.9999\n",
      "Epoch 18/200\n",
      " - 28s - loss: 0.1027 - post_attention_LSTM_dense_softmax_loss: 0.0103 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9992 - post_attention_LSTM_dense_softmax_acc_6: 0.9992 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9987 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 19/200\n",
      " - 28s - loss: 0.0950 - post_attention_LSTM_dense_softmax_loss: 0.0090 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9993 - post_attention_LSTM_dense_softmax_acc_6: 0.9993 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9989 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 20/200\n",
      " - 28s - loss: 0.0889 - post_attention_LSTM_dense_softmax_loss: 0.0081 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9993 - post_attention_LSTM_dense_softmax_acc_6: 0.9995 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9993 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 21/200\n",
      " - 27s - loss: 0.0835 - post_attention_LSTM_dense_softmax_loss: 0.0072 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9995 - post_attention_LSTM_dense_softmax_acc_6: 0.9995 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9994 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 22/200\n",
      " - 28s - loss: 0.0789 - post_attention_LSTM_dense_softmax_loss: 0.0066 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9995 - post_attention_LSTM_dense_softmax_acc_6: 0.9996 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9996 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 23/200\n",
      " - 28s - loss: 0.0750 - post_attention_LSTM_dense_softmax_loss: 0.0059 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9996 - post_attention_LSTM_dense_softmax_acc_6: 0.9997 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9996 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 24/200\n",
      " - 27s - loss: 0.0713 - post_attention_LSTM_dense_softmax_loss: 0.0054 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9997 - post_attention_LSTM_dense_softmax_acc_6: 0.9998 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9998 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 25/200\n",
      " - 28s - loss: 0.0684 - post_attention_LSTM_dense_softmax_loss: 0.0049 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9997 - post_attention_LSTM_dense_softmax_acc_6: 0.9999 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9998 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 26/200\n",
      " - 28s - loss: 0.0657 - post_attention_LSTM_dense_softmax_loss: 0.0045 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9998 - post_attention_LSTM_dense_softmax_acc_6: 0.9999 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9998 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 27/200\n",
      " - 28s - loss: 0.0632 - post_attention_LSTM_dense_softmax_loss: 0.0041 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9998 - post_attention_LSTM_dense_softmax_acc_6: 0.9999 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9998 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 28/200\n",
      " - 28s - loss: 0.0611 - post_attention_LSTM_dense_softmax_loss: 0.0038 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9999 - post_attention_LSTM_dense_softmax_acc_6: 0.9999 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 0.9999 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 29/200\n",
      " - 28s - loss: 0.0592 - post_attention_LSTM_dense_softmax_loss: 0.0035 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9999 - post_attention_LSTM_dense_softmax_acc_6: 0.9999 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 30/200\n",
      " - 28s - loss: 0.0574 - post_attention_LSTM_dense_softmax_loss: 0.0033 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9999 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 31/200\n",
      " - 28s - loss: 0.0557 - post_attention_LSTM_dense_softmax_loss: 0.0030 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9999 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 32/200\n",
      " - 28s - loss: 0.0542 - post_attention_LSTM_dense_softmax_loss: 0.0029 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 33/200\n",
      " - 28s - loss: 0.0529 - post_attention_LSTM_dense_softmax_loss: 0.0027 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9999 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 34/200\n",
      " - 28s - loss: 0.0516 - post_attention_LSTM_dense_softmax_loss: 0.0025 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 0.9999 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 35/200\n",
      " - 28s - loss: 0.0505 - post_attention_LSTM_dense_softmax_loss: 0.0023 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 36/200\n",
      " - 28s - loss: 0.0495 - post_attention_LSTM_dense_softmax_loss: 0.0022 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 37/200\n",
      " - 28s - loss: 0.0484 - post_attention_LSTM_dense_softmax_loss: 0.0021 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 38/200\n",
      " - 28s - loss: 0.0476 - post_attention_LSTM_dense_softmax_loss: 0.0020 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 39/200\n",
      " - 28s - loss: 0.0467 - post_attention_LSTM_dense_softmax_loss: 0.0019 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 40/200\n",
      " - 28s - loss: 0.0458 - post_attention_LSTM_dense_softmax_loss: 0.0018 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 41/200\n",
      " - 28s - loss: 0.0452 - post_attention_LSTM_dense_softmax_loss: 0.0017 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 42/200\n",
      " - 28s - loss: 0.0444 - post_attention_LSTM_dense_softmax_loss: 0.0016 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 43/200\n",
      " - 28s - loss: 0.0437 - post_attention_LSTM_dense_softmax_loss: 0.0015 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 44/200\n",
      " - 28s - loss: 0.0431 - post_attention_LSTM_dense_softmax_loss: 0.0014 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 45/200\n",
      " - 28s - loss: 0.0425 - post_attention_LSTM_dense_softmax_loss: 0.0014 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 46/200\n",
      " - 28s - loss: 0.0420 - post_attention_LSTM_dense_softmax_loss: 0.0013 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 47/200\n",
      " - 28s - loss: 0.0414 - post_attention_LSTM_dense_softmax_loss: 0.0013 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 48/200\n",
      " - 28s - loss: 0.0410 - post_attention_LSTM_dense_softmax_loss: 0.0012 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 49/200\n",
      " - 28s - loss: 0.0404 - post_attention_LSTM_dense_softmax_loss: 0.0012 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 50/200\n",
      " - 28s - loss: 0.0399 - post_attention_LSTM_dense_softmax_loss: 0.0011 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 51/200\n",
      " - 28s - loss: 0.0394 - post_attention_LSTM_dense_softmax_loss: 0.0011 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 52/200\n",
      " - 28s - loss: 0.0390 - post_attention_LSTM_dense_softmax_loss: 0.0011 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 53/200\n",
      " - 28s - loss: 0.0387 - post_attention_LSTM_dense_softmax_loss: 0.0010 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 54/200\n",
      " - 28s - loss: 0.0382 - post_attention_LSTM_dense_softmax_loss: 9.9581e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 55/200\n",
      " - 28s - loss: 0.0379 - post_attention_LSTM_dense_softmax_loss: 9.5363e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 56/200\n",
      " - 28s - loss: 0.0375 - post_attention_LSTM_dense_softmax_loss: 9.4709e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 57/200\n",
      " - 28s - loss: 0.0370 - post_attention_LSTM_dense_softmax_loss: 8.9973e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 58/200\n",
      " - 28s - loss: 0.0367 - post_attention_LSTM_dense_softmax_loss: 8.8283e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 59/200\n",
      " - 28s - loss: 0.0364 - post_attention_LSTM_dense_softmax_loss: 8.6822e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 60/200\n",
      " - 28s - loss: 0.0360 - post_attention_LSTM_dense_softmax_loss: 8.4278e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 61/200\n",
      " - 28s - loss: 0.0356 - post_attention_LSTM_dense_softmax_loss: 8.2403e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 62/200\n",
      " - 28s - loss: 0.0353 - post_attention_LSTM_dense_softmax_loss: 7.9819e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 63/200\n",
      " - 28s - loss: 0.0349 - post_attention_LSTM_dense_softmax_loss: 7.8094e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 64/200\n",
      " - 28s - loss: 0.0345 - post_attention_LSTM_dense_softmax_loss: 7.6579e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 65/200\n",
      " - 28s - loss: 0.0344 - post_attention_LSTM_dense_softmax_loss: 7.4616e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 66/200\n",
      " - 28s - loss: 0.0340 - post_attention_LSTM_dense_softmax_loss: 7.3560e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 67/200\n",
      " - 28s - loss: 0.0338 - post_attention_LSTM_dense_softmax_loss: 7.2388e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 68/200\n",
      " - 28s - loss: 0.0336 - post_attention_LSTM_dense_softmax_loss: 7.0658e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 69/200\n",
      " - 28s - loss: 0.0332 - post_attention_LSTM_dense_softmax_loss: 7.1070e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 70/200\n",
      " - 28s - loss: 0.0329 - post_attention_LSTM_dense_softmax_loss: 6.7188e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9998 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 71/200\n",
      " - 28s - loss: 0.0328 - post_attention_LSTM_dense_softmax_loss: 6.6748e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 72/200\n",
      " - 28s - loss: 0.0325 - post_attention_LSTM_dense_softmax_loss: 6.5744e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 73/200\n",
      " - 28s - loss: 0.0322 - post_attention_LSTM_dense_softmax_loss: 6.4073e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 74/200\n",
      " - 28s - loss: 0.0319 - post_attention_LSTM_dense_softmax_loss: 6.4090e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 75/200\n",
      " - 28s - loss: 0.0318 - post_attention_LSTM_dense_softmax_loss: 6.2328e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 76/200\n",
      " - 28s - loss: 0.0315 - post_attention_LSTM_dense_softmax_loss: 6.1671e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 77/200\n",
      " - 28s - loss: 0.0314 - post_attention_LSTM_dense_softmax_loss: 5.9638e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 78/200\n",
      " - 28s - loss: 0.0313 - post_attention_LSTM_dense_softmax_loss: 6.0054e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9998 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 79/200\n",
      " - 28s - loss: 0.0310 - post_attention_LSTM_dense_softmax_loss: 5.8354e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 80/200\n",
      " - 28s - loss: 0.0308 - post_attention_LSTM_dense_softmax_loss: 5.7904e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 81/200\n",
      " - 28s - loss: 0.0306 - post_attention_LSTM_dense_softmax_loss: 5.7214e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 82/200\n",
      " - 28s - loss: 0.0305 - post_attention_LSTM_dense_softmax_loss: 5.6193e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9998 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 83/200\n",
      " - 28s - loss: 0.0304 - post_attention_LSTM_dense_softmax_loss: 5.4993e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9998 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 84/200\n",
      " - 28s - loss: 0.0300 - post_attention_LSTM_dense_softmax_loss: 5.5004e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 85/200\n",
      " - 28s - loss: 0.0301 - post_attention_LSTM_dense_softmax_loss: 5.4070e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 86/200\n",
      " - 28s - loss: 0.0297 - post_attention_LSTM_dense_softmax_loss: 5.3657e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 87/200\n",
      " - 28s - loss: 0.0295 - post_attention_LSTM_dense_softmax_loss: 5.4317e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 88/200\n",
      " - 28s - loss: 0.0295 - post_attention_LSTM_dense_softmax_loss: 5.2164e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 89/200\n",
      " - 28s - loss: 0.0293 - post_attention_LSTM_dense_softmax_loss: 5.1180e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 90/200\n",
      " - 28s - loss: 0.0292 - post_attention_LSTM_dense_softmax_loss: 5.0845e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 91/200\n",
      " - 28s - loss: 0.0292 - post_attention_LSTM_dense_softmax_loss: 5.0138e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 92/200\n",
      " - 28s - loss: 0.0289 - post_attention_LSTM_dense_softmax_loss: 4.9421e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 93/200\n",
      " - 28s - loss: 0.0288 - post_attention_LSTM_dense_softmax_loss: 4.9567e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 94/200\n",
      " - 28s - loss: 0.0286 - post_attention_LSTM_dense_softmax_loss: 4.8781e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 95/200\n",
      " - 28s - loss: 0.0285 - post_attention_LSTM_dense_softmax_loss: 4.8291e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 96/200\n",
      " - 28s - loss: 0.0286 - post_attention_LSTM_dense_softmax_loss: 4.8061e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 97/200\n",
      " - 28s - loss: 0.0282 - post_attention_LSTM_dense_softmax_loss: 4.7106e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 98/200\n",
      " - 28s - loss: 0.0281 - post_attention_LSTM_dense_softmax_loss: 4.6965e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 99/200\n",
      " - 28s - loss: 0.0280 - post_attention_LSTM_dense_softmax_loss: 4.6331e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 100/200\n",
      " - 28s - loss: 0.0278 - post_attention_LSTM_dense_softmax_loss: 4.6128e-04 - post_attention_LSTM_dense_softmax_acc: 0.9998 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 101/200\n",
      " - 28s - loss: 0.0278 - post_attention_LSTM_dense_softmax_loss: 4.5809e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 102/200\n",
      " - 28s - loss: 0.0276 - post_attention_LSTM_dense_softmax_loss: 4.5046e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 103/200\n",
      " - 28s - loss: 0.0276 - post_attention_LSTM_dense_softmax_loss: 4.4639e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 104/200\n",
      " - 28s - loss: 0.0273 - post_attention_LSTM_dense_softmax_loss: 4.4266e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 105/200\n",
      " - 28s - loss: 0.0274 - post_attention_LSTM_dense_softmax_loss: 4.4362e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 106/200\n",
      " - 28s - loss: 0.0271 - post_attention_LSTM_dense_softmax_loss: 4.3957e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 107/200\n",
      " - 28s - loss: 0.0271 - post_attention_LSTM_dense_softmax_loss: 4.3300e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 108/200\n",
      " - 28s - loss: 0.0272 - post_attention_LSTM_dense_softmax_loss: 4.3033e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9955 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 109/200\n",
      " - 28s - loss: 0.0270 - post_attention_LSTM_dense_softmax_loss: 4.3205e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 110/200\n",
      " - 28s - loss: 0.0267 - post_attention_LSTM_dense_softmax_loss: 4.2050e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 111/200\n",
      " - 28s - loss: 0.0266 - post_attention_LSTM_dense_softmax_loss: 4.2069e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 112/200\n",
      " - 28s - loss: 0.0265 - post_attention_LSTM_dense_softmax_loss: 4.1243e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 113/200\n",
      " - 28s - loss: 0.0263 - post_attention_LSTM_dense_softmax_loss: 4.1377e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 114/200\n",
      " - 28s - loss: 0.0264 - post_attention_LSTM_dense_softmax_loss: 4.1169e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 115/200\n",
      " - 28s - loss: 0.0262 - post_attention_LSTM_dense_softmax_loss: 4.0907e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 116/200\n",
      " - 28s - loss: 0.0263 - post_attention_LSTM_dense_softmax_loss: 4.0366e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 117/200\n",
      " - 28s - loss: 0.0261 - post_attention_LSTM_dense_softmax_loss: 4.0009e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 118/200\n",
      " - 28s - loss: 0.0259 - post_attention_LSTM_dense_softmax_loss: 4.0476e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 119/200\n",
      " - 28s - loss: 0.0260 - post_attention_LSTM_dense_softmax_loss: 4.0074e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 120/200\n",
      " - 28s - loss: 0.0260 - post_attention_LSTM_dense_softmax_loss: 3.9085e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 121/200\n",
      " - 28s - loss: 0.0258 - post_attention_LSTM_dense_softmax_loss: 3.9408e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 122/200\n",
      " - 28s - loss: 0.0256 - post_attention_LSTM_dense_softmax_loss: 3.8966e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 123/200\n",
      " - 28s - loss: 0.0255 - post_attention_LSTM_dense_softmax_loss: 3.8478e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 124/200\n",
      " - 28s - loss: 0.0254 - post_attention_LSTM_dense_softmax_loss: 3.8169e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 125/200\n",
      " - 28s - loss: 0.0254 - post_attention_LSTM_dense_softmax_loss: 3.8404e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 126/200\n",
      " - 28s - loss: 0.0253 - post_attention_LSTM_dense_softmax_loss: 3.7832e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 127/200\n",
      " - 28s - loss: 0.0253 - post_attention_LSTM_dense_softmax_loss: 3.7888e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 128/200\n",
      " - 28s - loss: 0.0251 - post_attention_LSTM_dense_softmax_loss: 3.7500e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 129/200\n",
      " - 28s - loss: 0.0251 - post_attention_LSTM_dense_softmax_loss: 3.7253e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 130/200\n",
      " - 28s - loss: 0.0249 - post_attention_LSTM_dense_softmax_loss: 3.7039e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 131/200\n",
      " - 28s - loss: 0.0249 - post_attention_LSTM_dense_softmax_loss: 3.7988e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 132/200\n",
      " - 27s - loss: 0.0249 - post_attention_LSTM_dense_softmax_loss: 3.6678e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 133/200\n",
      " - 27s - loss: 0.0247 - post_attention_LSTM_dense_softmax_loss: 3.6704e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 134/200\n",
      " - 27s - loss: 0.0246 - post_attention_LSTM_dense_softmax_loss: 3.6419e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 135/200\n",
      " - 27s - loss: 0.0246 - post_attention_LSTM_dense_softmax_loss: 3.6098e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 136/200\n",
      " - 27s - loss: 0.0245 - post_attention_LSTM_dense_softmax_loss: 3.6016e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 137/200\n",
      " - 27s - loss: 0.0245 - post_attention_LSTM_dense_softmax_loss: 3.5675e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 138/200\n",
      " - 27s - loss: 0.0244 - post_attention_LSTM_dense_softmax_loss: 3.5706e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 139/200\n",
      " - 27s - loss: 0.0242 - post_attention_LSTM_dense_softmax_loss: 3.5273e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 140/200\n",
      " - 27s - loss: 0.0242 - post_attention_LSTM_dense_softmax_loss: 3.5463e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 141/200\n",
      " - 27s - loss: 0.0242 - post_attention_LSTM_dense_softmax_loss: 3.5204e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 142/200\n",
      " - 27s - loss: 0.0239 - post_attention_LSTM_dense_softmax_loss: 3.5002e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 143/200\n",
      " - 27s - loss: 0.0241 - post_attention_LSTM_dense_softmax_loss: 3.4812e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9956 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 144/200\n",
      " - 27s - loss: 0.0240 - post_attention_LSTM_dense_softmax_loss: 3.4635e-04 - post_attention_LSTM_dense_softmax_acc: 1.0000 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 145/200\n",
      " - 27s - loss: 0.0239 - post_attention_LSTM_dense_softmax_loss: 3.4447e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 146/200\n",
      " - 27s - loss: 0.0239 - post_attention_LSTM_dense_softmax_loss: 3.4516e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 147/200\n",
      " - 27s - loss: 0.0237 - post_attention_LSTM_dense_softmax_loss: 3.4242e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 148/200\n",
      " - 27s - loss: 0.0236 - post_attention_LSTM_dense_softmax_loss: 3.4109e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 149/200\n",
      " - 27s - loss: 0.0235 - post_attention_LSTM_dense_softmax_loss: 3.3645e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 150/200\n",
      " - 27s - loss: 0.0235 - post_attention_LSTM_dense_softmax_loss: 3.4007e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 151/200\n",
      " - 27s - loss: 0.0235 - post_attention_LSTM_dense_softmax_loss: 3.3402e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 152/200\n",
      " - 27s - loss: 0.0235 - post_attention_LSTM_dense_softmax_loss: 3.3386e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 153/200\n",
      " - 27s - loss: 0.0234 - post_attention_LSTM_dense_softmax_loss: 3.3367e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 154/200\n",
      " - 27s - loss: 0.0233 - post_attention_LSTM_dense_softmax_loss: 3.3196e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 155/200\n",
      " - 27s - loss: 0.0233 - post_attention_LSTM_dense_softmax_loss: 3.3269e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 156/200\n",
      " - 27s - loss: 0.0232 - post_attention_LSTM_dense_softmax_loss: 3.2805e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 157/200\n",
      " - 27s - loss: 0.0231 - post_attention_LSTM_dense_softmax_loss: 3.2777e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 158/200\n",
      " - 27s - loss: 0.0230 - post_attention_LSTM_dense_softmax_loss: 3.2697e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 159/200\n",
      " - 27s - loss: 0.0230 - post_attention_LSTM_dense_softmax_loss: 3.2545e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 160/200\n",
      " - 27s - loss: 0.0229 - post_attention_LSTM_dense_softmax_loss: 3.2395e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 161/200\n",
      " - 27s - loss: 0.0229 - post_attention_LSTM_dense_softmax_loss: 3.2164e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 162/200\n",
      " - 27s - loss: 0.0228 - post_attention_LSTM_dense_softmax_loss: 3.2210e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 163/200\n",
      " - 27s - loss: 0.0227 - post_attention_LSTM_dense_softmax_loss: 3.2188e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 164/200\n",
      " - 27s - loss: 0.0227 - post_attention_LSTM_dense_softmax_loss: 3.1781e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 165/200\n",
      " - 27s - loss: 0.0226 - post_attention_LSTM_dense_softmax_loss: 3.1643e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 166/200\n",
      " - 27s - loss: 0.0226 - post_attention_LSTM_dense_softmax_loss: 3.1692e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 167/200\n",
      " - 27s - loss: 0.0225 - post_attention_LSTM_dense_softmax_loss: 3.2027e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 168/200\n",
      " - 27s - loss: 0.0224 - post_attention_LSTM_dense_softmax_loss: 3.1319e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 169/200\n",
      " - 27s - loss: 0.0224 - post_attention_LSTM_dense_softmax_loss: 3.1388e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 170/200\n",
      " - 27s - loss: 0.0223 - post_attention_LSTM_dense_softmax_loss: 3.1229e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 171/200\n",
      " - 27s - loss: 0.0222 - post_attention_LSTM_dense_softmax_loss: 3.1143e-04 - post_attention_LSTM_dense_softmax_acc: 1.0000 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 172/200\n",
      " - 27s - loss: 0.0221 - post_attention_LSTM_dense_softmax_loss: 3.0931e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 173/200\n",
      " - 27s - loss: 0.0222 - post_attention_LSTM_dense_softmax_loss: 3.0815e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 174/200\n",
      " - 27s - loss: 0.0222 - post_attention_LSTM_dense_softmax_loss: 3.0844e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 175/200\n",
      " - 27s - loss: 0.0219 - post_attention_LSTM_dense_softmax_loss: 3.0763e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 176/200\n",
      " - 27s - loss: 0.0220 - post_attention_LSTM_dense_softmax_loss: 3.0571e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 177/200\n",
      " - 27s - loss: 0.0222 - post_attention_LSTM_dense_softmax_loss: 3.0541e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 178/200\n",
      " - 27s - loss: 0.0220 - post_attention_LSTM_dense_softmax_loss: 3.0371e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 179/200\n",
      " - 27s - loss: 0.0218 - post_attention_LSTM_dense_softmax_loss: 3.0236e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 180/200\n",
      " - 27s - loss: 0.0217 - post_attention_LSTM_dense_softmax_loss: 3.0342e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 181/200\n",
      " - 26s - loss: 0.0217 - post_attention_LSTM_dense_softmax_loss: 2.9997e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 182/200\n",
      " - 26s - loss: 0.0218 - post_attention_LSTM_dense_softmax_loss: 3.0049e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 183/200\n",
      " - 27s - loss: 0.0217 - post_attention_LSTM_dense_softmax_loss: 3.0064e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 184/200\n",
      " - 27s - loss: 0.0216 - post_attention_LSTM_dense_softmax_loss: 2.9831e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 185/200\n",
      " - 27s - loss: 0.0216 - post_attention_LSTM_dense_softmax_loss: 2.9605e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 186/200\n",
      " - 27s - loss: 0.0215 - post_attention_LSTM_dense_softmax_loss: 2.9604e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9957 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 187/200\n",
      " - 27s - loss: 0.0215 - post_attention_LSTM_dense_softmax_loss: 2.9365e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 188/200\n",
      " - 27s - loss: 0.0214 - post_attention_LSTM_dense_softmax_loss: 2.9406e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 189/200\n",
      " - 27s - loss: 0.0215 - post_attention_LSTM_dense_softmax_loss: 2.9497e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 190/200\n",
      " - 26s - loss: 0.0211 - post_attention_LSTM_dense_softmax_loss: 2.9261e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 191/200\n",
      " - 26s - loss: 0.0213 - post_attention_LSTM_dense_softmax_loss: 2.8964e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9997 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 192/200\n",
      " - 27s - loss: 0.0213 - post_attention_LSTM_dense_softmax_loss: 2.9047e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 193/200\n",
      " - 27s - loss: 0.0212 - post_attention_LSTM_dense_softmax_loss: 2.8946e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 194/200\n",
      " - 27s - loss: 0.0211 - post_attention_LSTM_dense_softmax_loss: 2.8958e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 195/200\n",
      " - 27s - loss: 0.0211 - post_attention_LSTM_dense_softmax_loss: 2.8714e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 196/200\n",
      " - 27s - loss: 0.0211 - post_attention_LSTM_dense_softmax_loss: 2.8610e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 197/200\n",
      " - 27s - loss: 0.0209 - post_attention_LSTM_dense_softmax_loss: 2.8515e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 198/200\n",
      " - 27s - loss: 0.0209 - post_attention_LSTM_dense_softmax_loss: 2.8836e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9959 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 199/200\n",
      " - 27s - loss: 0.0208 - post_attention_LSTM_dense_softmax_loss: 2.8458e-04 - post_attention_LSTM_dense_softmax_acc: 1.0000 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n",
      "Epoch 200/200\n",
      " - 27s - loss: 0.0210 - post_attention_LSTM_dense_softmax_loss: 2.8186e-04 - post_attention_LSTM_dense_softmax_acc: 0.9999 - post_attention_LSTM_dense_softmax_acc_1: 0.9999 - post_attention_LSTM_dense_softmax_acc_2: 0.9996 - post_attention_LSTM_dense_softmax_acc_3: 0.9958 - post_attention_LSTM_dense_softmax_acc_4: 1.0000 - post_attention_LSTM_dense_softmax_acc_5: 1.0000 - post_attention_LSTM_dense_softmax_acc_6: 1.0000 - post_attention_LSTM_dense_softmax_acc_7: 1.0000 - post_attention_LSTM_dense_softmax_acc_8: 1.0000 - post_attention_LSTM_dense_softmax_acc_9: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = [train_x, s0, c0], y = train_y, batch_size = 200, epochs = 200, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CY_QunyBLVhR",
    "outputId": "02b25cab-22c4-46c8-f81e-ed194b62d291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 69s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# since y is a list of Ty Numpy arrays, which means the model has multiple \n",
    "# outputs, the returned value is also a list of Ty scalars.\n",
    "s_init = np.zeros((m, n_s))\n",
    "c_init = np.zeros((m, n_s))\n",
    "preds = model.evaluate(x = [train_x, s_init, c_init], y = train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "I05jtQ-eLVhU",
    "outputId": "9a9683f8-b005-4890-ef46-9f3e1826a106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "[0.999925, 0.999875, 0.9996, 0.99595, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy for each of the Ty time steps\n",
    "print(\"Training Accuracy:\")\n",
    "print(preds[11:]) # check model.metrics_names for the meaning of each value in preds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04rUGUIILVhY"
   },
   "source": [
    "## 5. Model Testing\n",
    "\n",
    "We generate some random data and test the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUZHGVx0LVha"
   },
   "outputs": [],
   "source": [
    "# function to test examples\n",
    "\n",
    "def test(examples):\n",
    "    \"\"\"\n",
    "    Test some examples.\n",
    "    \n",
    "    Argument:\n",
    "    examples: a list of tuples of (human readable date, machine readable date).\n",
    "    \"\"\"\n",
    "    \n",
    "    human_readable, machine_readable = zip(*examples)\n",
    "    m = len(human_readable)\n",
    "    s_initial = np.zeros((m, n_s))\n",
    "    c_initial = np.zeros((m, n_s))\n",
    "    predictions = [] # the predicted machine readable dates\n",
    "    \n",
    "    for example in human_readable:\n",
    "        example_index = string_to_ints(example.lower(), Tx, human_vocab)\n",
    "        example_oh = np.array(list(map(lambda x : to_categorical(x, num_classes = len(human_vocab)), example_index)))\n",
    "        example_oh = np.expand_dims(example_oh, 0) # insert axis = 0, so the dimension is (1, Tx, len(human_vocab))\n",
    "        prediction = model.predict(x = [example_oh, s_initial, c_initial])\n",
    "        prediction = np.argmax(prediction, axis = -1)\n",
    "        prediction = [inv_machine_vocab[i] for i in prediction.flatten().tolist()]\n",
    "        prediction = ''.join(prediction)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    y = np.array(machine_readable)\n",
    "    y_pred = np.array(predictions)\n",
    "    acc = np.sum(y == y_pred) / len(y)\n",
    "    print('Test accuracy: ' + str(acc))\n",
    "    \n",
    "    # show 10 prediction examples\n",
    "    print('\\nSome translation results:')\n",
    "    for i in range(10):\n",
    "        print(human_readable[i] + ' ---> ' + predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "sNUnvh1GLVhd",
    "outputId": "36374ed1-6578-48d0-e490-c01fa9199808"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 20870.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.996\n",
      "\n",
      "Some translation results:\n",
      " may 26 10 ---> 2010-05-26\n",
      " 22 nov 1983 ---> 1983-11-22\n",
      "1/28/78 ---> 1978-01-28\n",
      "wednesday sep 04 13 ---> 2013-09-04\n",
      "sun 31 may 09 ---> 2009-05-31\n",
      "wed 18 feb 81 ---> 1981-02-18\n",
      "tuesday 02 september 80 ---> 1980-09-02\n",
      "monday september 11 78 ---> 1978-09-11\n",
      "thu 12 jul 01 ---> 2001-07-12\n",
      "tuesday july 10 79 ---> 1979-07-10\n"
     ]
    }
   ],
   "source": [
    "examples, _, _, _ = generate_dataset(1000)\n",
    "test(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPglyo3BLVhf"
   },
   "source": [
    "## 6. Visualizing Attention\n",
    "\n",
    "One advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can  visualize what part of the output is looking at what part of the input.\n",
    "\n",
    "We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X48IvftLVhg"
   },
   "outputs": [],
   "source": [
    "def plot_attention_map(model, Tx, Ty, model_layer, human_vocab, inv_machine_vocab, example):\n",
    "    \"\"\"\n",
    "    Visualize the attention values. model_layer is the layer# of the model for outputting attention weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_map = np.zeros((Ty, Tx)) # attantion map matrix to be plotted\n",
    "    s_initial = np.zeros((1, n_s))\n",
    "    c_initial = np.zeros((1, n_s))\n",
    "    \n",
    "    example_index = string_to_ints(example.lower(), Tx, human_vocab)\n",
    "    example_oh = np.array(list(map(lambda x : to_categorical(x, num_classes = len(human_vocab)), example_index)))\n",
    "    example_oh = np.expand_dims(example_oh, 0) # insert axis = 0, so the dimension is (1, Tx, len(human_vocab))\n",
    "   \n",
    "    layer = model.layers[model_layer]\n",
    "    func = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "    attention_weights = func([example_oh, s_initial, c_initial]) # get attention weights, shape (Ty, 1, Tx, 1)\n",
    "    \n",
    "    # fill in attention_map\n",
    "    for t in range(Ty):\n",
    "        for t_prime in range(Tx):\n",
    "            attention_map[t][t_prime] = attention_weights[t][0][t_prime][0]\n",
    "    \n",
    "    # get the predicted string, as a list of characters\n",
    "    prediction = model.predict(x = [example_oh, s_initial, c_initial])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    prediction = [inv_machine_vocab[i] for i in prediction.flatten().tolist()]\n",
    "    \n",
    "    # lengths of the input and output strings to be plotted\n",
    "    out_length = len(prediction)\n",
    "    in_length = len(example)\n",
    "    \n",
    "    # prepare the figure\n",
    "    figure = plt.figure(figsize = (8, 8.5))\n",
    "    ax = figure.add_subplot(1, 1, 1)\n",
    "    img = ax.imshow(attention_map, interpolation = 'nearest', cmap = 'Blues')\n",
    "    ax.set_yticks(range(out_length))\n",
    "    ax.set_yticklabels(prediction)\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "    ax.set_xticks(range(in_length))\n",
    "    ax.set_xticklabels(example, rotation = 45)\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.grid()\n",
    "    # add colorbar\n",
    "    cbaraxes = figure.add_axes([0.13, 0.28, 0.75, 0.03]) # [left, bottom, width, height] \n",
    "    figure.colorbar(mappable = img, cax = cbaraxes, orientation = 'horizontal')\n",
    "    cbaraxes.set_xlabel('color bar')\n",
    "    \n",
    "    print('Input: ' + example)\n",
    "    print('Output: ' + ''.join(prediction))\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "qgGl38NgLVhj",
    "outputId": "da268b97-f34f-41f8-8c68-6a44cf742b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Wednesday Jul 9 2009\n",
      "Output: 2009-07-09\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEDCAYAAAAV5CxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns5rzYG1mZlZzHqzNzMxqzoO1mZlZzf1/pCs7slJzKasAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, Tx, Ty, 7, human_vocab, inv_machine_vocab, 'Wednesday Jul 9 2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "K_rK_RCnLVhl",
    "outputId": "f6e7f5ea-7d4f-47ca-da4b-73aa47d20b04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 10/15/1988\n",
      "Output: 1988-10-15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEDCAYAAAAV5CxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSs7F2szMrORcrM3MzErOxdrMzKzk/j8Wq25xP3jDtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, Tx, Ty, 7, human_vocab, inv_machine_vocab, '10/15/1988')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "fEXSBjzZLVhn",
    "outputId": "13315681-4d3f-493b-c0f1-75343764fd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: may 26 10\n",
      "Output: 2010-05-26\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEDCAYAAAAV5CxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWcm5WJuZmZWci7WZmVnJuVibmZmV3P8HVgp2h3mDtg0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, Tx, Ty, 7, human_vocab, inv_machine_vocab, 'may 26 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "YlnxdmxKLVhs",
    "outputId": "5db1bc7c-eda6-4296-ac77-5aef5aceb29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 10.15.88\n",
      "Output: 1988-10-15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEDCAYAAAAV5CxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7OS+/9LmRqXXvPoKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, Tx, Ty, 7, human_vocab, inv_machine_vocab, '10.15.88')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2kA2B94t1Ov"
   },
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "K2EGrf9ct4wF",
    "outputId": "7f4ecf64-6064-4042-97af-83333cdac8fd"
   },
   "outputs": [],
   "source": [
    "model.save('nmt_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Machine Translation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
