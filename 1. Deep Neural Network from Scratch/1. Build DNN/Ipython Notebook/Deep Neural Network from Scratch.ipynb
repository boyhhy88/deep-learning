{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this project, we will build a deep neural network from scratch. \n",
    "\n",
    "The network consists of L layers (including the input layer). The last layer uses sigmoid activation, while the others use ReLU activation.\n",
    "\n",
    "The training dataset is split into **mini batches**, which seeks to find a balance between stochastic gradient descent and batch gradient descent. \n",
    "\n",
    "Three **initialization** methods are used: random initialization (standard normal distribution), He initialization, and Xavier initialization.\n",
    "\n",
    "Two **regularization** techniques are implemented: inverted dropout and L2 regularization.\n",
    "\n",
    "Three **optimization** algorithms are applied: gradient descent, Momentum, and Adam.\n",
    "\n",
    "With **gradient checking**, we are able to numerically check the derivatives computed by our code to make sure that the implementation is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages and Set Default Parameters\n",
    "\n",
    "- [numpy](http://www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- util_func provides some necessary functions for the calculations, e.g., Sigmoid, RELU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util_func import *\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outline of the Deep Neural Network\n",
    "\n",
    "The procedures to implement the deep neural network is the following:\n",
    "- Initialize the parameters (weight matrices W and bias vectors b) for an $L$-layer neural network.\n",
    "- Implement forward propagation (shown in purple in the figure below):\n",
    "     - For each layer $l$, complete the LINEAR part ($Z^{[l]}$).\n",
    "     - Compute the ACTIVATION function (relu/sigmoid).\n",
    "     - Stack the [LINEAR->RELU] forward function $L$-1 times (for layers 1 through $L$-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$).\n",
    "- Compute the cost.\n",
    "- Implement the backward propagation (shown in red in the figure below):\n",
    "    - For each layer $l$, compute $dZ^{[l]}$ using $dA^{[l]}$ from the last step.\n",
    "    - compute $dW^{[l]}$, $db^{[l]}$ and $dA^{[l - 1]}$ using $dZ^{[l]}$. \n",
    "    - Repeat $L$ times backward. \n",
    "- Update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/outline.png\" style=\"width:800px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions to complete the steps above are included in this notebook.\n",
    "\n",
    "**Note** that in the codes, the total number of layers, L, includes the input layer (l = 0). \n",
    "\n",
    "**Note** that for every forward propagation, there is a corresponding backward propagation. That is why at forward propagation we will be storing some values in a cache. In the backpropagation propagation we will then use the cache to calculate the gradients. \n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialization\n",
    "\n",
    "### 3.1 Gradient Descent\n",
    "\n",
    "We should make sure that our dimensions match between each layer. Recall that $n^{[l]}$ is the number of units in layer $l$. Thus for example if the size of our input $X$ is $(12288, 209)$ (with $m=209$ examples) then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/matrix dimensions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that when we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix}\\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{1}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "- We will store $n^{[l]}$, the number of units in different layers, in a variable `layer_dims`. For example, if `layer_dims`is [2,4,1]: There are two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. \n",
    "- Use zeros initialization for the biases.\n",
    "\n",
    "**A well chosen initialization can:**\n",
    "- Speed up the convergence of gradient descent\n",
    "- Increase the odds of gradient descent converging to a lower training (and generalization) error \n",
    "\n",
    "**We can use three methods to initialize the weight matrices:**\n",
    "  - Random initialization -- setting init_method = \"random_normal\" in the input argument. This initializes the weights to random values with standard normal distribution.\n",
    "  - He initialization -- setting init_method = \"he\" in the input argument. This initializes the weights to random values with a scaling factor of `sqrt(2/dimension of the previous layer)`, which is what He initialization recommends for layers with a ReLU activation.\n",
    "  - Xavier initialization -- setting init_method = \"xavier\" in the input argument. This initializes the weights to random values with a scaling factor of `sqrt(1/dimension of the previous layer)`, which is recommended for layers with a tanh activation.\n",
    "\n",
    "**Note** that in general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and the network is no more powerful than a linear classifier such as logistic regression. Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters: weight matrices and bias vectors for each layer\n",
    "\n",
    "def init_params(layer_dims, init_method, seed = 0):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    layer_dims: python array, layer_dims[l] is the number of units in the lth layer. \n",
    "                l = 0 is the input layer, the last l is the output layer.\n",
    "    init_method: choose which initialization method to use: \n",
    "                \"random_normal\":  random values following standard normal distribution.\n",
    "                \"he\": He initialization\n",
    "                \"xavier\": Xavier initialization\n",
    "    seed: random seed\n",
    "    \n",
    "    Returns:\n",
    "    params: python dictionary containing weight matrices wl and bias vectors bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "            Wl has the dimension of (layer_dims[l], layer_dims[l - 1]).\n",
    "            bl has the dimension of (layer_dims[l], 1)\n",
    "            \n",
    "    Use random initialization for the weight matrices, and use zeros initialization for the biases.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    params = {} # parameters to be returned\n",
    "    \n",
    "    L = len(layer_dims) # total number of layers, including the input layer. \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        if init_method == \"random_normal\":\n",
    "            params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 10\n",
    "        elif init_method == \"he\":\n",
    "            params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        elif init_method == \"xavier\":\n",
    "             params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1 / layer_dims[l-1])\n",
    "        params['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return params      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Momentum\n",
    "\n",
    "Initialize the velocities for Momentum optimization. See Section 8.2 in this notebook for details.\n",
    "\n",
    "$$v_{dW} = \\beta * v_{dW} + (1-\\beta) * dW\\tag{3}$$\n",
    "\n",
    "$$v_{db} = \\beta * v_{db} + (1-\\beta) * db\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize velocities for momentum optimization\n",
    "\n",
    "def init_momentum(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    layer_dims: python array, layer_dims[l] is the number of units in the lth layer. \n",
    "                l = 0 is the input layer, the last l is the output layer.\n",
    "    \n",
    "    Returns:\n",
    "    v: python dictionary containing velocities for each layer, \n",
    "            v['dW1'], v['dW2'], ..., v['dWl'], ..., v['db1'], v['db2'], ..., v['dbl'], ...  \n",
    "            v['dWl'] has the dimension of (layer_dims[l], layer_dims[l - 1])\n",
    "            v['dbl'] has the dimension of (layer_dims[l], 1)\n",
    "            \n",
    "    Use zero initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    v = {}\n",
    "    \n",
    "    L = len(layer_dims) # total number of layers, including the input layer. \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        v['dW' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "        v['db' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return v      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Adam Optimization\n",
    "\n",
    "Initialize the velocities and squared gradients for Adam optimization. See Section 8.3 in this notebook for details.\n",
    "\n",
    "$$v_{dW} = \\beta_1 * v_{dW} + (1-\\beta_1) * dW\\tag{5}$$\n",
    "\n",
    "$$v_{db} = \\beta_1 * v_{db} + (1-\\beta_1) * db\\tag{6}$$\n",
    "\n",
    "$$S_{dW} = \\beta_2 * S_{dW} + (1-\\beta_2) * dW^{2}\\tag{7}$$\n",
    "\n",
    "$$S_{db} = \\beta_2 * S_{db} + (1-\\beta_2) * db^{2}\\tag{8}$$\n",
    "\n",
    "$$v_{dW}^{corrected} = v_{dW}/(1-\\beta_1^{t}\\tag{9})$$\n",
    "\n",
    "$$v_{db}^{corrected} = v_{db}/(1-\\beta_1^{t}\\tag{10})$$\n",
    "\n",
    "$$S_{dW}^{corrected} = S_{dW}/(1-\\beta_2^{t}\\tag{11})$$\n",
    "\n",
    "$$S_{db}^{corrected} = S_{db}/(1-\\beta_2^{t}\\tag{12})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize v and s for Adam optimization\n",
    "\n",
    "def init_adam(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    layer_dims: python array, layer_dims[l] is the number of units in the lth layer. \n",
    "                l = 0 is the input layer, the last l is the output layer.\n",
    "    \n",
    "    Returns:\n",
    "    v: python dictionary containing velocities for each layer, \n",
    "            v['dW1'], v['dW2'], ..., v['dWl'], ..., v['db1'], v['db2'], ..., v['dbl'], ... \n",
    "            v['dWl'] has the dimension of (layer_dims[l], layer_dims[l - 1]).\n",
    "            v['dbl'] has the dimension of (layer_dims[l], 1).\n",
    "    s: python dictionary containing squared gradients for each layer, \n",
    "            s['dW1'], s['dW2'], ..., s['dWl'], ...,  s['db1'], s['db2'], ..., s['dbl'], ...  \n",
    "            s['dWl'] has the dimension of (layer_dims[l], layer_dims[l - 1]).        \n",
    "            s['dbl'] has the dimension of (layer_dims[l], 1)\n",
    "            \n",
    "    Use zero initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    L = len(layer_dims) # total number of layers, including the input layer. \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        v['dW' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "        v['db' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        s['dW' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "        s['db' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return v, s     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mini-Batch Gradient Descent\n",
    "\n",
    "### 4.1 Batch Gradient Descent\n",
    "When we take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent. \n",
    "\n",
    "### 4.2 Stochastic Gradient Descent (SGD)\n",
    "A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. When the training set is large, SGD can be faster. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stochastic gradient descent.png\" style=\"width:750px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing SGD requires 3 for-loops in total:\n",
    "1. Over the number of iterations\n",
    "2. Over the $m$ training examples\n",
    "3. Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L-1]},b^{[L-1]})$)\n",
    "\n",
    "\n",
    "### 4.3 Mini-Batch Gradient Descent\n",
    "In practice, we'll often get faster results if we use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/minibatch.png\" style=\"width:750px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.\n",
    "\n",
    "There are two steps:\n",
    "- **Shuffle**: Create a shuffled version of the training set (X, Y). Note that the random shuffling is done synchronously between X and Y, such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/minibatch_shuffle.png\" style=\"width:550px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Partition**: Partition the shuffled (X, Y) into mini-batches of size `mini_batch_size`. Note that the number of training examples is not always divisible by `mini_batch_size`. The last mini batch might be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/minibatch_partition.png\" style=\"width:550px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mini batches randomly, each with the size of mini_batch_size.\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size, seed = 0):\n",
    "    \"\"\"    \n",
    "    Argument:\n",
    "    X: input features, with dimension of (number of features, number of training examples)\n",
    "    Y: true labels, the actual y values, with the dimension of (1, number of training examples)\n",
    "    mini_batch_size: size of each mini batch.\n",
    "    seed: random seed\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches: list of (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "        \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    m = X.shape[1] # total number of examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # for batch gradient descent, no need to shuffle\n",
    "    if mini_batch_size == X.shape[1]:\n",
    "        mini_batch = (X, Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        return mini_batches\n",
    "    \n",
    "    # Step 1: shuffle\n",
    "    permutation_indices = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation_indices]\n",
    "    shuffled_Y = Y[:, permutation_indices]\n",
    "    \n",
    "    # Step 2: partition\n",
    "    num_batch_except_last = math.floor(m / mini_batch_size)\n",
    "    for i in range(num_batch_except_last):\n",
    "        mini_batch_X = shuffled_X[:, i * mini_batch_size:(i + 1) * mini_batch_size - 1]\n",
    "        mini_batch_Y = shuffled_Y[:, i * mini_batch_size:(i + 1) * mini_batch_size - 1]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    # last mini batch\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_batch_except_last * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_batch_except_last * mini_batch_size:]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Propagation Module\n",
    "\n",
    "### 5.1 Linear Forward \n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the following equations:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{13}$$\n",
    "\n",
    "where $A^{[0]} = X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Z value for forward propagation\n",
    "\n",
    "def linear_forward_Z(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev: the activations of the previous layer, with the dimension of (number of units in the previous layer, number of training examples)\n",
    "    W: weight matrix for current layer\n",
    "    b: bias vector for current layer\n",
    "    \n",
    "    Returns: \n",
    "    Z: the input of current layer's activation function, also called pre-activation parameter, Z[l] = W[l]A[l - 1] + b[l],\n",
    "        with the dimension of (number of units in current layer, number of training examples)\n",
    "    linear_cache: (A_prev, W, b), stored for calculating backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    \n",
    "    # verify the dimension correctness\n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    return Z, linear_cache    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Linear-Activation Forward\n",
    "\n",
    "In this notebook, we use two activation functions:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\frac{1}{ 1 + e^{-(Z)}}$. The `sigmoid` function is provided in util_func.py. \n",
    "\n",
    "- **ReLU**: $RELU(Z) = max(0, Z)$. The `relu` function is provided in util_func.py.\n",
    "\n",
    "We group two functions (Linear and Activation) into one function (LINEAR->ACTIVATION),\n",
    "\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or relu(). \n",
    "\n",
    "**Note**: In deep learning, the \"[LINEAR->ACTIVATION]\" computation is counted as a single layer in the neural network, not two layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the activation function for current layer\n",
    "\n",
    "def linear_forward_activation(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev: the activations of the previous layer, with the dimension of (number of units in the previous layer, number of training examples)\n",
    "    W: weight matrix for current layer\n",
    "    b: bias vector for current layer\n",
    "    activation: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    A: the output of current layer's activation function, also called post-activation value,\n",
    "        with the dimension of (number of units in current layer, number of training examples)\n",
    "    cache: ((A_prev, W, b), Z) stored for calculating backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward_Z(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, Z)\n",
    "    \n",
    "    return A, cache   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 L-Layer Forward Propagation \n",
    "\n",
    "#### 5.3.1 Without Dropout\n",
    "\n",
    "When implementing the $L$-layer Neural Net, we need a function that replicates the previous one (`linear_forward_activation` with RELU) $L-1$ times, then follows that with one `linear_forward_activation` with SIGMOID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/forward propagation.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the variable `Aout` will denote $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also called `Y hat`, i.e., $\\hat{Y}$.) \n",
    "\n",
    "The code below provides a full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing our predictions. It also records all intermediate values in \"caches\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole forward propagation of L-layer deep neural network, without dropout.\n",
    "# The output layer uses sigmoid activation, other layers use relu activation.\n",
    "# L includes the input layer\n",
    "\n",
    "def L_layer_forward(X, params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input features, with dimension of (number of features, number of training examples)\n",
    "    params: python dictionary containing weight matrices wl and bias vectors bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "            \n",
    "    Returns:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    caches: the list of cache from linear_forward_activation(A_prev, W, b, activation) (totally L - 1 of them since L counts the input layer, \n",
    "    indexed from 0 to L - 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    \n",
    "    # Division (/) always returns a float. To do floor division and get an integer result (discarding any fractional result), \n",
    "    # you can use the // operator. \n",
    "    L = len(params) // 2 + 1 # total number of layers including the input layer\n",
    "    \n",
    "    A_prev = X\n",
    "    \n",
    "    # relu activation for the layers except the last one\n",
    "    for l in range(1, L - 1):\n",
    "        W = params['W' + str(l)]\n",
    "        b = params['b' + str(l)]\n",
    "        A, cache = linear_forward_activation(A_prev, W, b, \"relu\")     \n",
    "        caches.append(cache)        \n",
    "        A_prev = A\n",
    "        \n",
    "    # sigmoid activation for the last layer\n",
    "    Aout, cache = linear_forward_activation(A_prev, params['W' + str(L - 1)], params['b' + str(L - 1)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(Aout.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return Aout, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Inverted Dropout\n",
    "\n",
    "**Dropout** is a widely used regularization technique that is specific to deep learning.**It randomly shuts down some neurons in each iteration.**\n",
    "\n",
    "At each iteration, we shut down (= set to zero) each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$. The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. \n",
    "\n",
    "To understand drop-out, consider this conversation with a friend:\n",
    "- Friend: \"Why do you need all these neurons to train your network and classify images?\". \n",
    "- You: \"Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!\"\n",
    "- Friend: \"I see, but are you sure that your neurons are learning different features and not all the same features?\"\n",
    "- You: \"Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<center>\n",
       "    <video width=\"620\" height=\"440\" controls>\n",
       "        <source src=\"videos/dropout1.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "</center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''\n",
    "<center>\n",
    "    <video width=\"620\" height=\"440\" controls>\n",
    "        <source src=\"videos/dropout1.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</center>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<center>\n",
       "    <video width=\"620\" height=\"440\" controls>\n",
       "        <source src=\"videos/dropout2.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "</center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''\n",
    "<center>\n",
    "    <video width=\"620\" height=\"440\" controls>\n",
    "        <source src=\"videos/dropout2.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</center>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we shut some neurons down, we actually modify our model. The idea behind drop-out is that at each iteration, we train a different model that uses only a subset of our neurons. With dropout, our neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. \n",
    "\n",
    "**We carry out 4 Steps:**\n",
    "1. Create a random matrix $D^{[1]} = [d^{[1](1)}, d^{[1](2)} ... d^{[1](m)}] $ of the same dimension as $A^{[1]}$.\n",
    "2. Set each entry of $D^{[1]}$ to be 0 with probability (`1-keep_prob`) or 1 with probability (`keep_prob`), by thresholding values in $D^{[1]}$ appropriately.\n",
    "3. Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. (shutting down some neurons). We can think of $D^{[1]}$ as a mask.\n",
    "4. Divide $A^{[1]}$ by `keep_prob`. By doing this we are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole forward propagation of L-layer deep neural network, with inverted dropout.\n",
    "# The output layer uses sigmoid activation, other layers use relu activation.\n",
    "# L includes the input layer\n",
    "\n",
    "def L_layer_forward_inverted_dropout(X, params, keep_prob, seed = 0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input features, with dimension of (number of features, number of training examples)\n",
    "    params: python dictionary containing weight matrices wl and bias vectors bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    keep_prob: parameter for inverted dropout,the probability of keeping a neuron\n",
    "    seed: random seed\n",
    "    \n",
    "    Returns:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    caches: the list of cache from linear_forward_activation(A_prev, W, b, activation) (totally L - 1 of them since L counts the input layer, \n",
    "    indexed from 0 to L - 2)\n",
    "    dropout_masks: the list of mask matrices for shutting down neurons in each layer (except the input and output layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    caches = []\n",
    "    dropout_masks = []\n",
    "    \n",
    "    L = len(params) // 2 + 1 # total number of layers including the input layer\n",
    "    \n",
    "    A_prev = X\n",
    "    \n",
    "    # relu activation for the layers except the last one\n",
    "    for l in range(1, L - 1):\n",
    "        W = params['W' + str(l)]\n",
    "        b = params['b' + str(l)]\n",
    "        A, cache = linear_forward_activation(A_prev, W, b, \"relu\")\n",
    "        caches.append(cache)        \n",
    "         \n",
    "        #mask = np.random.rand(A.shape[0], A.shape[1]) # random samples from a uniform distribution over [0, 1)\n",
    "        #If you want an interface that takes a shape-tuple as the first argument, refer to np.random.random_sample\n",
    "        mask = np.random.random_sample(A.shape)\n",
    "        \n",
    "        # to set all the entries of a matrix X to 0 (if entry is less than 0.4) or 1 (if entry is more than 0.4) you would do: X = (X < 0.4).\n",
    "        # Note that 0 and 1 are respectively equivalent to False and True.\n",
    "        mask = (mask < keep_prob)\n",
    "        A = A * mask\n",
    "        A = A / keep_prob\n",
    "        dropout_masks.append(mask)\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "    # sigmoid activation for the last layer\n",
    "    Aout, cache = linear_forward_activation(A_prev, params['W' + str(L - 1)], params['b' + str(L - 1)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(Aout.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return Aout, caches, dropout_masks     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost Function\n",
    "\n",
    "### 6.1 Non-Regularized\n",
    "\n",
    "Compute the cross-entropy cost $J$, using the following formula:\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{14}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cross-entropy cost function\n",
    "\n",
    "def cost_func(Aout, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    Y: true labels, the actual y values, with the dimension of (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost: the cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # numpy.multiply(): multiply arguments element-wise.Equivalent to x1 * x2 in terms of array broadcasting.\n",
    "    # numpy.nansum(): treating Not a Numbers (NaNs) as zero.\n",
    "    cost = -1 / m * np.nansum(np.multiply(Y, np.log(Aout)) + np.multiply(1 - Y, np.log(1 - Aout)), axis = 1, keepdims = True)\n",
    "    cost = np.squeeze(cost) # e.g. this turns [[5]] into 5\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 L2 Regularization\n",
    "\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{15}$$\n",
    "\n",
    "**What is L2-regularization actually doing?**\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function we drive all the weights to smaller values. It becomes too costly for the cost to have large weights. This leads to a smoother model in which the output changes more slowly as the input changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cost function with L2 regularization.\n",
    "\n",
    "def cost_func_L2_Regul(Aout, Y, params, lambd):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    Y: true labels, the actual y values, with the dimension of (1, number of training examples)\n",
    "    params: python dictionary containing weight matrices wl and bias vectors bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    lambd: the lambda parameter for L2 regularization\n",
    "    \n",
    "    Returns:\n",
    "    cost: the cost value\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]    \n",
    "    cross_entropy_cost = -1 / m * np.nansum(np.multiply(Y, np.log(Aout)) + np.multiply(1 - Y, np.log(1 - Aout)), axis = 1, keepdims = True)\n",
    "    \n",
    "    L = len(params) // 2 + 1\n",
    "    L2_regularization_cost = 0 \n",
    "    for l in range(1, L):\n",
    "        L2_regularization_cost += np.nansum(np.square(params['W' + str(l)]))\n",
    "    L2_regularization_cost *= lambd / (2 * m)\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost   \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Backward Propagation Module\n",
    "\n",
    "Back propagation is used to calculate the gradient of the loss function with respect to the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backprop.png\" style=\"width:650px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{16}$$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, we use the previous chain rule and we do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step we multiply our current gradient by the gradient corresponding to the specific layer to get the gradient we want.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, we use the previous chain rule and we do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Linear Backward\n",
    "\n",
    "#### 7.1.1 Non-Regularized\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose we have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/linearback.png\" style=\"width:250px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{17}$$\n",
    "\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{18}$$\n",
    "\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{19}$$\n",
    "\n",
    "**Note** that in this calculation, dA and dZ are alwarys the derivatives with respect to the loss function NOT the cost function. We treat dA and dZ differently from dW and dB. Namely, dA and dZ are always computing the derivatives d$\\mathcal{L}$/dA and d$\\mathcal{L}$/dZ respectively, but dW and db are computing the derivatives dJ/dW and dJ/db. This is why we don't have the 1/m factor in the dA formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation for current layer, given dZ\n",
    "# In our calculation, dA and dZ are alwarys the derivatives with respect to the loss function NOT the cost function. \n",
    "# We treat dA and dZ differently from dW and dB. Namely, dA and dZ are always computing the derivatives dL/dA and dL/dZ respectively, \n",
    "# but dW and db are computing the derivatives dJ/dW and dJ/db.\n",
    "# This is why we don't have the 1/m factor in the dA formula\n",
    "\n",
    "def linear_backward_from_dZ(dZ, linear_cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ: gradient of the loss with respect to Z for current layer\n",
    "    linear_cache: (A_prev, W, b) stored from linear forward propagation for current layer\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradient of the loss with respect to A_prev, for current layer, with the same dimension of A_prev\n",
    "    dW: gradient of the cost with respect to W, for current layer, with the same dimension of W\n",
    "    db: gradient of the cost with respect to b, for current layer, with the same dimension of b \n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = dZ.shape[1]\n",
    "    \n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 L2 Regularization\n",
    "\n",
    "If L2 regularization is used, because we change the cost, we have to change backward propagation as well. All the gradients have to be computed with respect to this new cost. The changes only concern dW. For each, we have to add the regularization term's gradient ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation for current layer, given dZ, with L2 regularization\n",
    "\n",
    "def linear_backward_from_dZ_L2_Regul(dZ, linear_cache, lambd):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ: gradient of the loss with respect to Z for current layer\n",
    "    linear_cache: (A_prev, W, b) stored from forward propagation for current layer\n",
    "    lambd: the lambda parameter for L2 regularization\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradient of the loss with respect to A_prev, for current layer, with the same dimension of A_prev\n",
    "    dW: gradient of the cost with respect to W, for current layer, with the same dimension of W\n",
    "    db: gradient of the cost with respect to b, for current layer, with the same dimension of b \n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = dZ.shape[1]\n",
    "    \n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T) + (lambd / m) * W # this is the only difference between L2 and non-regularized\n",
    "    db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Linear-Activation Backward\n",
    "\n",
    "#### 7.2.1 Non-Regularized\n",
    "\n",
    "Next, we create a function that merges **`linear_backward_from_dZ`** and the backward step for the activation ($dA$).\n",
    "\n",
    "Two backward functions are provided in util_func.py:\n",
    "\n",
    "- **`sigmoid_backward(dA, Z)`**: Implements the backward propagation for SIGMOID unit. \n",
    "\n",
    "- **`relu_backward(dA, Z)`**: Implements the backward propagation for RELU unit. \n",
    "\n",
    "If $g(.)$ is the activation function, `sigmoid_backward` and `relu_backward` compute\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{20}$$.\n",
    "\n",
    "Then we just use $dZ^{[l]}$ as the input of above function, linear_backward_from_dZ, to get $dW^{[l]}, db^{[l]}, dA^{[l-1]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation for current layer, given dA\n",
    "\n",
    "def activation_backward_from_dA(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA: gradient of the loss with respect to A[l] for current layer l\n",
    "    cache: ((A_prev, W, b), Z) stored from forward propagation for current layer\n",
    "    activation: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradient of the loss with respect to A_prev (A[l - 1]), for current layer, with the same dimension of A_prev\n",
    "    dW: gradient of the cost with respect to W, for current layer, with the same dimension of W\n",
    "    db: gradient of the cost with respect to b, for current layer, with the same dimension of b \n",
    "    \"\"\"\n",
    "    linear_cache, Z = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward_from_dZ(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation for current layer, given dA, with L2 regularization\n",
    "\n",
    "def activation_backward_from_dA_L2_Regul(dA, cache, activation, lambd):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA: gradient of the loss with respect to A[l] for current layer l\n",
    "    cache: ((A_prev, W, b), Z) stored from forward propagation for current layer\n",
    "    activation: \"sigmoid\" or \"relu\"\n",
    "    lambd: the lambda parameter for L2 regularization\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradient of the loss with respect to A_prev (A[l - 1]), for current layer, with the same dimension of A_prev\n",
    "    dW: gradient of the cost with respect to W, for current layer, with the same dimension of W\n",
    "    db: gradient of the cost with respect to b, for current layer, with the same dimension of b \n",
    "    \"\"\"\n",
    "    linear_cache, Z = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward_from_dZ_L2_Regul(dZ, linear_cache, lambd)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 L-Layer Backward Propagation\n",
    "\n",
    "#### 7.3.1 Non-Regularized\n",
    "\n",
    "Now we implement the backward function for the whole network. Recall that when we implemented the `L_layer_forward` function, at each layer $l$, we stored a cache which contains $((A^{[l-1]}, W^{[l]}, b^{[l]}), Z^{[l]})$. In the back propagation module, we use those caches to compute the gradients. Therefore, we iterate through all the hidden layers backward, starting from layer $L$. On each step, we use the cached values for layer $l$ to backpropagate through layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/l_layer_backward.png\" style=\"width:450px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing backpropagation**:\n",
    "To backpropagate through this network, we know that the output is, \n",
    "$Aout = A^{[L]} = \\sigma(Z^{[L]})$. We thus need to compute `dAout` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "To do so, use this formula:\n",
    "```python\n",
    "dAout = - (np.divide(Y, Aout) - np.divide(1 - Y, 1 - Aout)) # derivative of loss with respect to Aout\n",
    "```\n",
    "\n",
    "We can now feed in `dAout` into the LINEAR->SIGMOID backward function. After that, we have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. We should store each dA, dW, and db in the grads dictionary.\n",
    "\n",
    "**Note:**\n",
    "We actually don't use the $dAout$ formula above in the code; instead, we calculate $dZ^{[L]}$ for the output layer using $Aout - Y$. This is because $Aout$ could sometimes be 1 due to computer mathematical limitation, so np.divide(1 - Y, 1 - Aout) becomes np.divide(1 - Y, 0) which causes problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole backward propagation of L-layer deep neural network\n",
    "# The output layer use sigmoid activation, other layers use relu activation.\n",
    "\n",
    "def L_layer_backward(Aout, Y, caches):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    Y: true labels, the actual y values, with the dimension of (1, number of training examples)\n",
    "    caches: the list of cache from L_layer_forward(X, params), i.e., list of ((A_prev, W, b), Z) \n",
    "            (totally L - 1 of them since L counts the input layer, indexed from 0 to L - 2)\n",
    "            \n",
    "    Returns: \n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) + 1 # total number of layers including the input layer\n",
    "    # gradient of the loss with respect to the output Aout\n",
    "    # numpy.divide(): Divide arguments element-wise. Equivalent to x1 / x2 in terms of array-broadcasting.\n",
    "    #dAout = - Y / Aout + (1 - Y) / (1 - Aout)\n",
    "    dZout = Aout - Y    \n",
    "    \n",
    "    # calculate gradients for the last layer using sigmoid activation\n",
    "    cache = caches[-1]\n",
    "    linear_cache, _ = cache\n",
    "    #dA_prev, dW, db = activation_backward_from_dA(dAout, cache, \"sigmoid\")   \n",
    "    dA_prev, dW, db = linear_backward_from_dZ(dZout, linear_cache) \n",
    "    grads['dW' + str(L - 1)] = dW\n",
    "    grads['db' + str(L - 1)] = db\n",
    " \n",
    "    # calculate gradients for all the previous layers using relu activation\n",
    "    for l in range(L - 3, -1, -1):\n",
    "        cache = caches[l]\n",
    "        dA_prev, dW, db = activation_backward_from_dA(dA_prev, cache, \"relu\")\n",
    "        grads['dW' + str(l + 1)] = dW\n",
    "        grads['db' + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2 L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole backward propagation of L-layer deep neural network, with L2 regularization\n",
    "# The output layer use sigmoid activation, other layers use relu activation.\n",
    "\n",
    "def L_layer_backward_L2_Regul(Aout, Y, caches, lambd):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    Y: true labels, the actual y values, with the dimension of (1, number of training examples)\n",
    "    caches: the list of cache from L_layer_forward(X, params), i.e., list of ((A_prev, W, b), Z) \n",
    "            (totally L - 1 of them since L counts the input layer, indexed from 0 to L - 2)\n",
    "    lambd: the lambda parameter for L2 regularization\n",
    "            \n",
    "    Returns: \n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) + 1 # total number of layers including the input layer\n",
    "    # gradient of the cost with respect to the output Aout\n",
    "    # dAout = - Y / Aout + (1 - Y) / (1 - Aout)\n",
    "    dZout = Aout - Y   \n",
    "    \n",
    "    # calculate gradients for the last layer using sigmoid activation\n",
    "    cache = caches[-1]\n",
    "    linear_cache, _ = cache\n",
    "    #dA_prev, dW, db = activation_backward_from_dA_L2_Regul(dAout, cache, \"sigmoid\", lambd) \n",
    "    dA_prev, dW, db = linear_backward_from_dZ_L2_Regul(dZout, linear_cache, lambd) \n",
    "    grads['dW' + str(L - 1)] = dW\n",
    "    grads['db' + str(L - 1)] = db\n",
    " \n",
    "    # calculate gradients for all the previous layers using relu activation\n",
    "    for l in range(L - 3, -1, -1):\n",
    "        cache = caches[l]\n",
    "        dA_prev, dW, db = activation_backward_from_dA_L2_Regul(dA_prev, cache, \"relu\", lambd) \n",
    "        grads['dW' + str(l + 1)] = dW\n",
    "        grads['db' + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3 Inverted Dropout\n",
    "\n",
    "We carry out 2 Steps:\n",
    "1. We had previously shut down some neurons during forward propagation, by applying a mask $D^{[l]}$ to $A^{[l]}$. In backpropagation, we shut down the same neurons, by reapplying the same mask $D^{[1]}$ to $dA^{[l]}$. \n",
    "2. During forward propagation, we had divided $A^{[l]}$ by `keep_prob`. In backpropagation, we have to divide $dA^{[l]}$ by `keep_prob` again (the calculus interpretation is that if $A^{[1]}$ is scaled by `keep_prob`, then its derivative $dA^{[1]}$ is also scaled by the same `keep_prob`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole backward propagation of L-layer deep neural network, with inverted dropout\n",
    "# The output layer use sigmoid activation, other layers use relu activation.\n",
    "\n",
    "def L_layer_backward_inverted_dropout(Aout, Y, caches, keep_prob, dropout_masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Aout: output activation, i.e., the predicted y value, with the dimension of (1, number of examples)\n",
    "    Y: true labels, the actual y values, with the dimension of (1, number of training examples)\n",
    "    caches: the list of cache from L_layer_forward(X, params), i.e., list of ((A_prev, W, b), Z) \n",
    "            (totally L - 1 of them since L counts the input layer, indexed from 0 to L - 2)\n",
    "    keep_prob: parameter for inverted dropout,the probability of keeping a neuron\n",
    "    dropout_masks: the mask matrices for shutting down neurons in each layer (except the input and output layers)\n",
    "            \n",
    "    Returns: \n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) + 1 # total number of layers including the input layer\n",
    "    # gradient of the cost with respect to the output Aout\n",
    "    # dAout = - Y / Aout + (1 - Y) / (1 - Aout)\n",
    "    dZout = Aout - Y \n",
    "    \n",
    "    # calculate gradients for the last layer using sigmoid activation\n",
    "    cache = caches[-1]\n",
    "    linear_cache, _ = cache\n",
    "    # dA_prev, dW, db = activation_backward_from_dA(dAout, cache, \"sigmoid\")\n",
    "    dA_prev, dW, db = linear_backward_from_dZ(dZout, linear_cache) \n",
    "    grads['dW' + str(L - 1)] = dW\n",
    "    grads['db' + str(L - 1)] = db\n",
    " \n",
    "    # calculate gradients for all the previous layers using relu activation\n",
    "    for l in range(L - 3, -1, -1):\n",
    "        cache = caches[l]\n",
    "        \n",
    "        mask = dropout_masks[l]\n",
    "        assert(mask.shape == dA_prev.shape)\n",
    "        dA_prev = mask * dA_prev\n",
    "        dA_prev = dA_prev / keep_prob\n",
    "        \n",
    "        dA_prev, dW, db = activation_backward_from_dA(dA_prev, cache, \"relu\")\n",
    "        grads['dW' + str(l + 1)] = dW\n",
    "        grads['db' + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Update Parameters\n",
    "\n",
    "### 8.1 Gradient Descent\n",
    "\n",
    "We update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{21}$$\n",
    "\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{22}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters including weight matrices and bias vectors\n",
    "\n",
    "def update_params(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params: python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ... \n",
    "    learning_rate: learning rate alpha\n",
    "    \n",
    "    Returns:\n",
    "    updated params            \n",
    "    \"\"\"\n",
    "    L = len(params) // 2 + 1 # total number of layers including the input layer\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        params['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "        \n",
    "    return params    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Momentum\n",
    "\n",
    "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n",
    "\n",
    "Momentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. We can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/opt_momentum.png\" style=\"width:400px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.\n",
    "\n",
    "The momentum update rule is, for $l = 1, ..., L$:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{23}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{24}$$\n",
    "\n",
    "where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters including weight matrices and bias vectors, with Momentum optimization\n",
    "\n",
    "def update_params_momentum(params, v, grads, learning_rate, beta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params: python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    v: python dictionary containing velocities for each layer, \n",
    "            v['dW1'], v['dW2'], ..., v['dWl'], ..., v['db1'], v['db2'], ..., v['dbl'], ... \n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ... \n",
    "    learning_rate: learning rate alpha\n",
    "    beta: hyperparameter for momentum optimization\n",
    "    \n",
    "    Returns:\n",
    "    updated params and velocities            \n",
    "    \"\"\"\n",
    "    L = len(params) // 2 + 1 # total number of layers including the input layer\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        v['dW' + str(l)] = beta * v['dW' + str(l)] + (1 - beta) * grads['dW' + str(l)]\n",
    "        v['db' + str(l)] = beta * v['db' + str(l)] + (1 - beta) * grads['db' + str(l)]\n",
    "        params['W' + str(l)] -= learning_rate * v['dW' + str(l)]\n",
    "        params['b' + str(l)] -= learning_rate * v['db' + str(l)]\n",
    "        \n",
    "    return params, v    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Adam Optimization\n",
    "\n",
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum. \n",
    "\n",
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}\\tag{25}$$\n",
    "\n",
    "where:\n",
    "- t counts the number of steps taken of Adam (starting from 1)\n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero\n",
    "\n",
    "Some advantages of Adam include:\n",
    "- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n",
    "- Usually works well even with little tuning of hyperparameters (except $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters including weight matrices and bias vectors, with Adam optimization\n",
    "\n",
    "def update_params_adam(params, v, s, t, grads, learning_rate, beta1, beta2, epsilon):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params: python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    v: python dictionary containing velocities for each layer, \n",
    "            v['dW1'], v['dW2'], ..., v['dWl'], ..., v['db1'], v['db2'], ..., v['dbl'], ... \n",
    "    s: python dictionary containing squared gradients for each layer, \n",
    "            s['dW1'], s['dW2'], ..., s['dWl'], ..., s['db1'], s['db2'], ..., s['dbl'], ...\n",
    "    t: current number of steps (starting from 1)\n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['bl'], ... \n",
    "    learning_rate: learning rate alpha\n",
    "    beta1, beta2: hyperparameters for Adam optimization\n",
    "    epsilon: hyperparameter preventing division by zero in Adam updates\n",
    "    \n",
    "    Returns:\n",
    "    updated params, exponentially weighted average of the past gradients and squares of the past gradients      \n",
    "    \"\"\"\n",
    "    L = len(params) // 2 + 1 # total number of layers including the input layer\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        v['dW' + str(l)] = beta1 * v['dW' + str(l)] + (1 - beta1) * grads['dW' + str(l)]\n",
    "        v_corrected['dW' + str(l)] = v['dW' + str(l)] / (1 - beta1 ** t)\n",
    "        \n",
    "        v['db' + str(l)] = beta1 * v['db' + str(l)] + (1 - beta1) * grads['db' + str(l)]\n",
    "        v_corrected['db' + str(l)] = v['db' + str(l)] / (1 - beta1 ** t)\n",
    "        \n",
    "        s['dW' + str(l)] = beta2 * s['dW' + str(l)] + (1 - beta2) * grads['dW' + str(l)] ** 2\n",
    "        s_corrected['dW' + str(l)] = s['dW' + str(l)] / (1 - beta2 ** t)\n",
    "        \n",
    "        s['db' + str(l)] = beta2 * s['db' + str(l)] + (1 - beta2) * grads['db' + str(l)] ** 2\n",
    "        s_corrected['db' + str(l)] = s['db' + str(l)] / (1 - beta2 ** t)\n",
    "        \n",
    "        params['W' + str(l)] -= learning_rate * v_corrected['dW' + str(l)] / (np.sqrt(s_corrected['dW' + str(l)]) + epsilon)\n",
    "        params['b' + str(l)] -= learning_rate * v_corrected['db' + str(l)] / (np.sqrt(s_corrected['db' + str(l)]) + epsilon)\n",
    "       \n",
    "    return params, v, s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. L-Layer Neural Network Complete Model\n",
    "\n",
    "Now we use the helper functions implemented above to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*.\n",
    "\n",
    "**Note**:\n",
    "- A **common mistake when using dropout** is to use it both in training and testing. We should use dropout (randomly eliminate nodes) only in training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the L-layer deep neural network model\n",
    "\n",
    "def L_layer_model(X, Y, layer_dims, params_seed = 0, mini_batch_size = None, optimizer = \"gd\", learning_rate = 0.0075, beta = 0.9, \n",
    "                  beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, iterations = 3300, print_cost_freq = 0, save_cost_freq = 100, \n",
    "                  initialization = \"he\", regularization = \"none\", lambd = 0, keep_prob = 1, dropout_seed = 0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: training set features, with the dimension of (number of features, number of examples)\n",
    "    Y: training set labels, with the dimension of (1, number of examples)\n",
    "    layer_dims: python array, layer_dims[l] is the number of units in the lth layer. \n",
    "                l = 0 is the input layer, the last l is the output layer\n",
    "    params_seed: random seed for initialization.\n",
    "    mini_batch_size: size of each mini batch. By default it is the total number of examples, i.e., batch gradient descent\n",
    "    optimizer: choose the optimization method:\n",
    "               \"gd\": gradient descent\n",
    "               \"momentum\": momentum optimization\n",
    "               \"adam\": Adam optimization\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    beta: hyperparameter for momentum optimization\n",
    "    beta1, beta2: hyperparameters for Adam optimization\n",
    "    epsilon: hyperparameter preventing division by zero in Adam updates\n",
    "    iterations: number of iterations\n",
    "    print_cost_freq: if > 0, print the cost value every print_cost_freq steps.\n",
    "    save_cost_freq: save the cost value every save_cost_freq steps into costs, for ploting the learning curve.\n",
    "    initialization: choose which initialization to use: \n",
    "                \"random_normal\":  random values following standard normal distribution.\n",
    "                \"he\": He initialization\n",
    "                \"xavier\": Xavier initialization\n",
    "    regularization: choose the regularization method:\n",
    "                \"none\": no regularization\n",
    "                \"L2\": L2 regularization\n",
    "    lambd: the lambda parameter for L2 regularization\n",
    "    keep_prob: keep_prob: parameter for inverted dropout,the probability of keeping a neuron. If it's 1, dropout is not used.\n",
    "    dropout_seed: random seed for dropout.\n",
    "    \n",
    "    Returns:\n",
    "    params: python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "            Wl has the dimension of (layer_dims[l], layer_dims[l - 1]).\n",
    "            bl has the dimension of (layer_dims[l], 1)\n",
    "            \n",
    "    Note: in this code, dropout and L2 regularization are not used at the same time\n",
    "    \"\"\"\n",
    "    assert(keep_prob == 1 or regularization == \"none\")\n",
    "    assert(keep_prob <= 1)\n",
    "    \n",
    "    if mini_batch_size is None:\n",
    "        mini_batch_size = X.shape[1]\n",
    "    \n",
    "    costs = []\n",
    "    seed_mini_batches = 10 # random seed for creating mini batches\n",
    "    t = 0 # count current number of steps\n",
    "    \n",
    "    # initialization\n",
    "    params = init_params(layer_dims, initialization, params_seed)\n",
    "    if optimizer == \"gd\":\n",
    "        pass\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = init_momentum(layer_dims)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = init_adam(layer_dims)\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        # create mini batches\n",
    "        seed_mini_batches += 1\n",
    "        mini_batches = random_mini_batches(X, Y, mini_batch_size, seed_mini_batches)\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            mini_batch_X, mini_batch_Y = mini_batch\n",
    "            \n",
    "            # forward propagation\n",
    "            if keep_prob < 1:\n",
    "                Aout, caches, dropout_masks = L_layer_forward_inverted_dropout(mini_batch_X, params, keep_prob, dropout_seed)\n",
    "            else:\n",
    "                Aout, caches = L_layer_forward(mini_batch_X, params)\n",
    "        \n",
    "            # cost\n",
    "            if regularization == \"L2\":\n",
    "                cost = cost_func_L2_Regul(Aout, mini_batch_Y, params, lambd)\n",
    "            else:\n",
    "                cost = cost_func(Aout, mini_batch_Y)    \n",
    "        \n",
    "            # backward propagation\n",
    "            if keep_prob < 1:\n",
    "                grads = L_layer_backward_inverted_dropout(Aout, mini_batch_Y, caches, keep_prob, dropout_masks)\n",
    "            elif regularization == \"none\":\n",
    "                grads = L_layer_backward(Aout, mini_batch_Y, caches)\n",
    "            elif regularization == \"L2\":\n",
    "                grads = L_layer_backward_L2_Regul(Aout, mini_batch_Y, caches, lambd)\n",
    "        \n",
    "            # update weight matrices and bias vectors\n",
    "            if optimizer == \"gd\":\n",
    "                params = update_params(params, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                params, v = update_params_momentum(params, v, grads, learning_rate, beta)\n",
    "            elif optimizer == \"adam\":\n",
    "                t += 1\n",
    "                params, v, s = update_params_adam(params, v, s, t, grads, learning_rate, beta1, beta2, epsilon)\n",
    "        \n",
    "        if print_cost_freq > 0 and (i == 1 or i % print_cost_freq == 0):\n",
    "            print(\"current iteration: \" + str(i) + \", cost: \" + str(cost))\n",
    "        if save_cost_freq > 0 and (i == 1 or i % save_cost_freq == 0):\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations * ' + str(print_cost_freq))\n",
    "    plt.title('learning rate = ' + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gradient Checking\n",
    "\n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and the loss function.\n",
    "\n",
    "Because forward propagation is relatively easy to implement, we assume we're computing the cost $J$ correctly. Thus, we can use our code for computing $J$ to verify the code for computing $\\frac{\\partial J}{\\partial \\theta}$. \n",
    "\n",
    "The definition of a derivative (or gradient):\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{26}$$\n",
    "\n",
    "**How does gradient checking work?**.\n",
    "\n",
    "We convert the \"parameters\" dictionary into a vector, obtained by reshaping all parameters (W1, b1, W2, b2, W3, b3, ....) into vectors and concatenating them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient checking.png\" style=\"width:600px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each parameter:\n",
    "- First compute \"gradapprox\" using the formula (26) above and a small value of $\\varepsilon$. Here are the Steps to follow:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- Then compute the gradient using backward propagation, and store the result in a variable \"grad\"\n",
    "\n",
    "Thus, we get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to `params_values[i]`. Finally, compute the relative difference between \"gradapprox\" and the \"grad\" using the following formula:\n",
    "\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{27}$$\n",
    "\n",
    "**Note** \n",
    "- Gradient Checking is slow! Approximating the gradient with $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ is computationally costly. For this reason, we don't run gradient checking at every iteration during training. Just a few times to check if the gradient is correct. \n",
    "- Gradient Checking, at least as we've presented it, doesn't work with dropout. We would usually run the gradient check algorithm without dropout to make sure our backprop is correct, then add dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert params to a vector\n",
    "\n",
    "def params_to_vector(params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params: python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    \n",
    "    Returns:\n",
    "    theta: one-column vector by flattening and concatenating params['W1'], params['b1'], params['W2'], ..., \n",
    "            and params['b2'], ..., params['Wl'], params['bl'], ...\n",
    "    keys: list of keys for each row of theta, ['W1', 'W1'...,'b1','b1',...]\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    L = len(params) // 2 + 1\n",
    "    first_flag = True\n",
    "    for l in range(1, L):\n",
    "        cur_vector = np.reshape(params['W' + str(l)], (-1, 1))\n",
    "        keys = keys + ['W' + str(l)] * cur_vector.shape[0]\n",
    "        if first_flag:\n",
    "            theta = cur_vector\n",
    "            first_flag = False\n",
    "        else:\n",
    "            theta = np.concatenate((theta, cur_vector), axis = 0)\n",
    "            \n",
    "        cur_vector = np.reshape(params['b' + str(l)], (-1, 1))\n",
    "        keys = keys + ['b' + str(l)] * cur_vector.shape[0]\n",
    "        theta = np.concatenate((theta, cur_vector), axis = 0)\n",
    "    \n",
    "    return theta, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert grads to a vector\n",
    "\n",
    "def grads_to_vector(grads):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ... \n",
    "    \n",
    "    Returns:\n",
    "    theta: one-column vector by flattening and concatenating grads['dW1'], grads['db1'], grads['dW2'], ..., \n",
    "            and grads['db2'], ..., grads['dWl'], grads['dbl'], ...\n",
    "    keys: list of keys for each row of theta, ['dW1', 'dW1'...,'db1','db1',...]\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    L = len(grads) // 2 + 1\n",
    "    first_flag = True\n",
    "    for l in range(1, L):\n",
    "        cur_vector = np.reshape(grads['dW' + str(l)], (-1, 1))\n",
    "        keys = keys + ['dW' + str(l)] * cur_vector.shape[0]\n",
    "        if first_flag:\n",
    "            theta = cur_vector\n",
    "            first_flag = False\n",
    "        else:\n",
    "            theta = np.concatenate((theta, cur_vector), axis = 0)\n",
    "            \n",
    "        cur_vector = np.reshape(grads['db' + str(l)], (-1, 1))\n",
    "        keys = keys + ['db' + str(l)] * cur_vector.shape[0]\n",
    "        theta = np.concatenate((theta, cur_vector), axis = 0)\n",
    "    \n",
    "    return theta, keys\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vector back to params\n",
    "\n",
    "def vector_to_params(theta, params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    theta: one-column vector by flattening and concatenating grads['dW1'], grads['db1'], grads['dW2'], ..., \n",
    "            and grads['db2'], ..., grads['dWl'], grads['dbl'], ...\n",
    "    params: original python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "            it has the information of the exact dimension of each matrix\n",
    "    \n",
    "    Returns:\n",
    "    params: converted weight matrices and bias vectors from theta\n",
    "    \"\"\"\n",
    "    L = len(params) // 2 + 1\n",
    "    index = 0;\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = theta[index : index + params['W' + str(l)].size, 0].reshape(params['W' + str(l)].shape)\n",
    "        index += params['W' + str(l)].size\n",
    "        params['b' + str(l)] = theta[index : index + params['b' + str(l)].size, 0].reshape(params['b' + str(l)].shape)\n",
    "        index += params['b' + str(l)].size\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checking\n",
    "\n",
    "def gradient_checking(params, grads, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    params: python dictionary containing weight matrix wl and bias vector bl for the lth layer, \n",
    "            params['W1'], params['W2'], ..., params['Wl'], ..., and params['b1'], params['b2'], ..., params['bl'], ... \n",
    "    grads: python dictionary containing gradients of the cost with respect to the weight matrices and bias vectors in each layer,\n",
    "            grads['dW1'], grads['dW2'], ..., grads['dWl'], ..., and grads['db1'], grads['db2'], ..., grads['dbl'], ..., \n",
    "            to be compared to \"gradapprox\"\n",
    "    X: data set features, with the dimension of (number of features, number of examples)\n",
    "    Y: data set labels, with the dimension of (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    difference: difference between the approximated gradient and the backward propagation gradient, defined above\n",
    "    \"\"\"\n",
    "    params_values, _ = params_to_vector(params)\n",
    "    grads_values, _ = grads_to_vector(grads)\n",
    "    num_params = params_values.shape[0]\n",
    "    J_plus = np.zeros(params_values.shape)\n",
    "    J_minus = np.zeros(params_values.shape)\n",
    "    \n",
    "    for i in range(num_params):\n",
    "        # get J_plus\n",
    "        params_values_copy = np.copy(params_values)\n",
    "        params_values_copy[i, 0] += epsilon\n",
    "        Aout, _ = L_layer_forward(X, vector_to_params(params_values_copy, params.copy()))\n",
    "        J_plus[i, 0] = cost_func(Aout, Y)\n",
    "        # get J_minus\n",
    "        params_values_copy = np.copy(params_values)\n",
    "        params_values_copy[i, 0] -= epsilon\n",
    "        Aout, _ = L_layer_forward(X, vector_to_params(params_values_copy, params.copy()))\n",
    "        J_minus[i, 0] = cost_func(Aout, Y)\n",
    "        \n",
    "    # get approximated gradients\n",
    "    gradapprox = np.subtract(J_plus, J_minus) / (2 * epsilon)\n",
    "    \n",
    "    # calculate difference\n",
    "    difference = np.linalg.norm(grads_values - gradapprox) / (np.linalg.norm(grads_values) + np.linalg.norm(gradapprox))\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        # font color:\n",
    "        # Red = '\\033[91m', Green = '\\033[92m', Blue = '\\033[94m', Cyan = '\\033[96m', White = '\\033[97m',\n",
    "        # Yellow = '\\033[93m', Magenta = '\\033[95m', Grey = '\\033[90m', Black = '\\033[90m', Default = '\\033[99m'\n",
    "        # end: '\\033[0m'\n",
    "        print('\\033[91m' + 'There is a mistake, the difference is ' + str(difference) + '\\033[0m')\n",
    "    else:\n",
    "        print('\\033[92m' + 'Good, the difference is ' + str(difference) + '\\033[0m')\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random values of X, Y and params as test case \n",
    "\n",
    "def gradient_checking_test():\n",
    "    np.random.seed(10)\n",
    "    X = np.random.randn(5, 4)\n",
    "    Y = np.array([[1, 1, 0, 0]])\n",
    "    W1 = np.random.randn(6, 5)\n",
    "    b1 = np.random.randn(6, 1)\n",
    "    W2 = np.random.randn(3, 6)\n",
    "    b2 = np.random.randn(3, 1)\n",
    "    W3 = np.random.randn(1, 3)\n",
    "    b3 = np.random.randn(1, 1)\n",
    "    params = {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2, 'W3':W3, 'b3':b3}\n",
    "    \n",
    "    return X, Y, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient checking\n",
    "def run_gradient_checking():\n",
    "    X, Y, params = gradient_checking_test()\n",
    "    Aout, caches = L_layer_forward(X, params)\n",
    "    grads = L_layer_backward(Aout, Y, caches)\n",
    "    gradient_checking(params, grads, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGood, the difference is 1.959878475377247e-09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_gradient_checking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
